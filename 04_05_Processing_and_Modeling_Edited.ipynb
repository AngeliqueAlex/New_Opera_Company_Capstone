{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb80c4b-3cf8-459b-b44e-f8bf5911030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix,recall_score , accuracy_score, precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de73d6eb-0f44-4958-bd4e-b102a5e698e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>iso</th>\n",
       "      <th>city</th>\n",
       "      <th>composer</th>\n",
       "      <th>db</th>\n",
       "      <th>dd</th>\n",
       "      <th>nat</th>\n",
       "      <th>mf</th>\n",
       "      <th>work</th>\n",
       "      <th>worknat</th>\n",
       "      <th>...</th>\n",
       "      <th>performances_season_by_city</th>\n",
       "      <th>perf_per_1k_ppl_city_pop</th>\n",
       "      <th>opera_by_composer</th>\n",
       "      <th>performances_season_by_country_total</th>\n",
       "      <th>performances_season_by_city_total</th>\n",
       "      <th>perf_total_per_1k_city_pop</th>\n",
       "      <th>perf_total_per_10k_co_pop</th>\n",
       "      <th>Season Year</th>\n",
       "      <th>country_change_from_previous_season</th>\n",
       "      <th>city_change_from_previous_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Lortzing</td>\n",
       "      <td>1801</td>\n",
       "      <td>1851</td>\n",
       "      <td>de</td>\n",
       "      <td>m</td>\n",
       "      <td>Ali Pascha von Janina</td>\n",
       "      <td>de</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>Ali Pascha von Janina by Lor</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.262847</td>\n",
       "      <td>0.385301</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Mozart</td>\n",
       "      <td>1756</td>\n",
       "      <td>1791</td>\n",
       "      <td>at</td>\n",
       "      <td>m</td>\n",
       "      <td>Don Giovanni</td>\n",
       "      <td>it</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>Don Giovanni by Moz</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.262847</td>\n",
       "      <td>0.385301</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Puccini</td>\n",
       "      <td>1858</td>\n",
       "      <td>1924</td>\n",
       "      <td>it</td>\n",
       "      <td>m</td>\n",
       "      <td>Tosca</td>\n",
       "      <td>it</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>Tosca by Puc</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.262847</td>\n",
       "      <td>0.385301</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1213</td>\n",
       "      <td>am</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Spendiaryan</td>\n",
       "      <td>1871</td>\n",
       "      <td>1928</td>\n",
       "      <td>am</td>\n",
       "      <td>m</td>\n",
       "      <td>Almast</td>\n",
       "      <td>am</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>Almast by Spe</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>0.101510</td>\n",
       "      <td>0.371147</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1213</td>\n",
       "      <td>am</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Tigranian</td>\n",
       "      <td>1879</td>\n",
       "      <td>1950</td>\n",
       "      <td>am</td>\n",
       "      <td>m</td>\n",
       "      <td>Anoush</td>\n",
       "      <td>am</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>Anoush by Tig</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>0.101510</td>\n",
       "      <td>0.371147</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season iso     city     composer    db    dd nat mf                   work  \\\n",
       "0    1213  al   Tirana     Lortzing  1801  1851  de  m  Ali Pascha von Janina   \n",
       "1    1213  al   Tirana       Mozart  1756  1791  at  m           Don Giovanni   \n",
       "2    1213  al   Tirana      Puccini  1858  1924  it  m                  Tosca   \n",
       "3    1213  am  Yerevan  Spendiaryan  1871  1928  am  m                 Almast   \n",
       "4    1213  am  Yerevan    Tigranian  1879  1950  am  m                 Anoush   \n",
       "\n",
       "  worknat  ... performances_season_by_city perf_per_1k_ppl_city_pop  \\\n",
       "0      de  ...                           9                 0.021506   \n",
       "1      it  ...                           9                 0.021506   \n",
       "2      it  ...                           9                 0.021506   \n",
       "3      am  ...                           5                 0.004573   \n",
       "4      am  ...                           5                 0.004573   \n",
       "\n",
       "              opera_by_composer performances_season_by_country_total  \\\n",
       "0  Ali Pascha von Janina by Lor                                  110   \n",
       "1           Don Giovanni by Moz                                  110   \n",
       "2                  Tosca by Puc                                  110   \n",
       "3                 Almast by Spe                                  111   \n",
       "4                 Anoush by Tig                                  111   \n",
       "\n",
       "   performances_season_by_city_total  perf_total_per_1k_city_pop  \\\n",
       "0                                110                    0.262847   \n",
       "1                                110                    0.262847   \n",
       "2                                110                    0.262847   \n",
       "3                                111                    0.101510   \n",
       "4                                111                    0.101510   \n",
       "\n",
       "  perf_total_per_10k_co_pop Season Year  country_change_from_previous_season  \\\n",
       "0                  0.385301  2013-01-01                                  NaN   \n",
       "1                  0.385301  2013-01-01                                  NaN   \n",
       "2                  0.385301  2013-01-01                                  NaN   \n",
       "3                  0.371147  2013-01-01                                  NaN   \n",
       "4                  0.371147  2013-01-01                                  NaN   \n",
       "\n",
       "   city_change_from_previous_season  \n",
       "0                               NaN  \n",
       "1                               NaN  \n",
       "2                               NaN  \n",
       "3                               NaN  \n",
       "4                               NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opera = pd.read_excel('/Users/angelique/Documents/GitHub/New_Opera_Company_Capstone/Excel and CSV Files/opera_4.xlsx')\n",
    "opera.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10859f5-efe4-4080-85cf-0d84b8c6035b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'iso', 'city', 'composer', 'db', 'dd', 'nat', 'mf', 'work',\n",
       "       'worknat', 'type', 'start date', 'performances', 'Country Name',\n",
       "       'city population', 'country population', 'continent', 'sub-region',\n",
       "       'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
       "       'performances_season_by_city', 'perf_per_1k_ppl_city_pop',\n",
       "       'opera_by_composer', 'performances_season_by_country_total',\n",
       "       'performances_season_by_city_total', 'perf_total_per_1k_city_pop',\n",
       "       'perf_total_per_10k_co_pop', 'Season Year',\n",
       "       'country_change_from_previous_season',\n",
       "       'city_change_from_previous_season'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opera.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90bcf431-c363-47ae-9b09-3781e993e7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season                                           int64\n",
       "iso                                             object\n",
       "city                                            object\n",
       "composer                                        object\n",
       "db                                              object\n",
       "dd                                              object\n",
       "nat                                             object\n",
       "mf                                              object\n",
       "work                                            object\n",
       "worknat                                         object\n",
       "type                                            object\n",
       "start date                              datetime64[ns]\n",
       "performances                                     int64\n",
       "Country Name                                    object\n",
       "city population                                  int64\n",
       "country population                             float64\n",
       "continent                                       object\n",
       "sub-region                                      object\n",
       "performances_season_by_country                   int64\n",
       "perf_per_10k_ppl_co_pop                        float64\n",
       "performances_season_by_city                      int64\n",
       "perf_per_1k_ppl_city_pop                       float64\n",
       "opera_by_composer                               object\n",
       "performances_season_by_country_total             int64\n",
       "performances_season_by_city_total                int64\n",
       "perf_total_per_1k_city_pop                     float64\n",
       "perf_total_per_10k_co_pop                      float64\n",
       "Season Year                             datetime64[ns]\n",
       "country_change_from_previous_season            float64\n",
       "city_change_from_previous_season               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opera.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c8b24f-6f63-4ba3-8431-b2274f1be84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>iso</th>\n",
       "      <th>city</th>\n",
       "      <th>composer</th>\n",
       "      <th>db</th>\n",
       "      <th>dd</th>\n",
       "      <th>nat</th>\n",
       "      <th>mf</th>\n",
       "      <th>work</th>\n",
       "      <th>worknat</th>\n",
       "      <th>...</th>\n",
       "      <th>perf_total_per_1k_city_pop</th>\n",
       "      <th>perf_total_per_10k_co_pop</th>\n",
       "      <th>Season Year</th>\n",
       "      <th>country_change_from_previous_season</th>\n",
       "      <th>city_change_from_previous_season</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Week_of_Year</th>\n",
       "      <th>Days_Since_Start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Lortzing</td>\n",
       "      <td>1801</td>\n",
       "      <td>1851</td>\n",
       "      <td>de</td>\n",
       "      <td>m</td>\n",
       "      <td>Ali Pascha von Janina</td>\n",
       "      <td>de</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262847</td>\n",
       "      <td>0.385301</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Mozart</td>\n",
       "      <td>1756</td>\n",
       "      <td>1791</td>\n",
       "      <td>at</td>\n",
       "      <td>m</td>\n",
       "      <td>Don Giovanni</td>\n",
       "      <td>it</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262847</td>\n",
       "      <td>0.385301</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Puccini</td>\n",
       "      <td>1858</td>\n",
       "      <td>1924</td>\n",
       "      <td>it</td>\n",
       "      <td>m</td>\n",
       "      <td>Tosca</td>\n",
       "      <td>it</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262847</td>\n",
       "      <td>0.385301</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1213</td>\n",
       "      <td>am</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Spendiaryan</td>\n",
       "      <td>1871</td>\n",
       "      <td>1928</td>\n",
       "      <td>am</td>\n",
       "      <td>m</td>\n",
       "      <td>Almast</td>\n",
       "      <td>am</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101510</td>\n",
       "      <td>0.371147</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1213</td>\n",
       "      <td>am</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Tigranian</td>\n",
       "      <td>1879</td>\n",
       "      <td>1950</td>\n",
       "      <td>am</td>\n",
       "      <td>m</td>\n",
       "      <td>Anoush</td>\n",
       "      <td>am</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101510</td>\n",
       "      <td>0.371147</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season iso     city     composer    db    dd nat mf                   work  \\\n",
       "0    1213  al   Tirana     Lortzing  1801  1851  de  m  Ali Pascha von Janina   \n",
       "1    1213  al   Tirana       Mozart  1756  1791  at  m           Don Giovanni   \n",
       "2    1213  al   Tirana      Puccini  1858  1924  it  m                  Tosca   \n",
       "3    1213  am  Yerevan  Spendiaryan  1871  1928  am  m                 Almast   \n",
       "4    1213  am  Yerevan    Tigranian  1879  1950  am  m                 Anoush   \n",
       "\n",
       "  worknat  ... perf_total_per_1k_city_pop perf_total_per_10k_co_pop  \\\n",
       "0      de  ...                   0.262847                  0.385301   \n",
       "1      it  ...                   0.262847                  0.385301   \n",
       "2      it  ...                   0.262847                  0.385301   \n",
       "3      am  ...                   0.101510                  0.371147   \n",
       "4      am  ...                   0.101510                  0.371147   \n",
       "\n",
       "   Season Year country_change_from_previous_season  \\\n",
       "0   2013-01-01                                 NaN   \n",
       "1   2013-01-01                                 NaN   \n",
       "2   2013-01-01                                 NaN   \n",
       "3   2013-01-01                                 NaN   \n",
       "4   2013-01-01                                 NaN   \n",
       "\n",
       "   city_change_from_previous_season  Year Month Weekday  Week_of_Year  \\\n",
       "0                               NaN  2013     3       5            12   \n",
       "1                               NaN  2013     5       5            20   \n",
       "2                               NaN  2013     2       2             7   \n",
       "3                               NaN  2013     7       3            28   \n",
       "4                               NaN  2013     5       5            19   \n",
       "\n",
       "   Days_Since_Start  \n",
       "0               273  \n",
       "1               329  \n",
       "2               235  \n",
       "3               383  \n",
       "4               322  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opera['Year'] = opera['start date'].dt.year\n",
    "opera['Month'] = opera['start date'].dt.month\n",
    "opera['Weekday'] = opera['start date'].dt.weekday\n",
    "opera['Week_of_Year'] = opera['start date'].dt.isocalendar().week\n",
    "opera['Days_Since_Start'] = (opera['start date'] - opera['start date'].min()).dt.days\n",
    "opera.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5135a0-3df9-4fa2-8a0e-14ae2702195b",
   "metadata": {},
   "source": [
    "I want to drop any columns that don't seem to be a factor or useful for any predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2350bdd4-f0d1-4c27-887d-c53a313a1587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'iso', 'city', 'work', 'start date', 'performances',\n",
       "       'Country Name', 'city population', 'country population', 'continent',\n",
       "       'sub-region', 'performances_season_by_country',\n",
       "       'perf_per_10k_ppl_co_pop', 'performances_season_by_city',\n",
       "       'perf_per_1k_ppl_city_pop', 'opera_by_composer',\n",
       "       'performances_season_by_country_total',\n",
       "       'performances_season_by_city_total', 'perf_total_per_1k_city_pop',\n",
       "       'perf_total_per_10k_co_pop', 'Season Year',\n",
       "       'country_change_from_previous_season',\n",
       "       'city_change_from_previous_season', 'Year', 'Month', 'Weekday',\n",
       "       'Week_of_Year', 'Days_Since_Start'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opera.drop(columns=['composer', 'db', 'dd', 'nat', 'mf', 'worknat', 'type'], inplace=True)\n",
    "opera.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275bbf2-cfa2-4161-9775-f2d675f7acdb",
   "metadata": {},
   "source": [
    "I'm creating a function to assess the top values for cities and countries. This function will call up lists similar to the graphs shown in my EDA component that showed the top cities and top countries solely based on the delta change in performances from season to season as a total sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e867f88-e87d-490a-9c56-d97d4fbe63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_top_values(df, group_column, value_column, top_n=25):\n",
    "    \"\"\"\n",
    "    This function ranks values (cities, countries, or any other category) based on the sum of a numeric column\n",
    "    and returns the top N rows with the highest sum of the specified value.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the data to be ranked.\n",
    "    - group_column (str): The column name to group by (e.g., 'city', 'country').\n",
    "    - value_column (str): The numeric column name that will be summed and used for ranking (e.g., 'growth').\n",
    "    - top_n (int): The number of top rows to return. Default is 25.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame of the top N values with the highest summed numeric column.\n",
    "    \"\"\"\n",
    "    # Group by the specified column and sum the values in the specified numeric column\n",
    "    grouped = df.groupby([group_column], as_index=False)[value_column].sum()\n",
    "\n",
    "    # Sort the grouped data by the value column in descending order (highest first)\n",
    "    sorted_group = grouped.sort_values(by=value_column, ascending=False)\n",
    "\n",
    "    # Get the top N rows based on the sorted values\n",
    "    top_values = sorted_group.head(top_n)\n",
    "\n",
    "    return top_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d4a4b96-f4f9-4e40-bbac-3252164e0ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               city  city_change_from_previous_season\n",
      "746          Moscow                           27911.0\n",
      "1213     Washington                           16128.0\n",
      "1059  St Petersburg                            9681.0\n",
      "51        Arlington                            9648.0\n",
      "262        Columbus                            3549.0\n",
      "757         München                            3381.0\n",
      "928        Richmond                            3042.0\n",
      "844           Paris                            2971.0\n",
      "865    Philadelphia                            2916.0\n",
      "326      Dusseldorf                            1677.0\n",
      "314         Dresden                            1674.0\n",
      "889     Portland OR                            1539.0\n",
      "62          Atlanta                            1312.0\n",
      "294          Denver                            1168.0\n",
      "369         Firenze                            1163.0\n",
      "1201       Voronezh                             979.0\n",
      "289          Dayton                             968.0\n",
      "536         Jackson                             900.0\n",
      "640            Linz                             866.0\n",
      "685          Madrid                             861.0\n",
      "458           Halle                             820.0\n",
      "381       Frankfurt                             817.0\n",
      "95        Barcelona                             765.0\n",
      "603       Lancaster                             720.0\n",
      "866         Phoenix                             720.0\n"
     ]
    }
   ],
   "source": [
    "top_cities = rank_top_values(opera, group_column='city', value_column='city_change_from_previous_season', top_n=25)\n",
    "print(top_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0ede498-2ab2-4104-aa35-10cd12d0561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Country Name  country_change_from_previous_season\n",
      "58  Russian Federation                             194425.0\n",
      "34               Italy                              87941.0\n",
      "65              Sweden                               6459.0\n",
      "55              Poland                               5653.0\n",
      "64               Spain                               3115.0\n",
      "50         Netherlands                               2892.0\n",
      "35               Japan                                631.0\n",
      "39              Latvia                                529.0\n",
      "23             Estonia                                517.0\n",
      "62            Slovenia                                509.0\n",
      "6              Belarus                                450.0\n",
      "73          Uzbekistan                                450.0\n",
      "2              Armenia                                442.0\n",
      "13               China                                396.0\n",
      "59              Serbia                                372.0\n",
      "1            Argentina                                288.0\n",
      "37  Korea, Republic of                                211.0\n",
      "49            Mongolia                                180.0\n",
      "32             Ireland                                177.0\n",
      "20             Denmark                                165.0\n",
      "0              Albania                                115.0\n",
      "26             Georgia                                111.0\n",
      "46              Mexico                                 91.0\n",
      "51         New Zealand                                 73.0\n",
      "36          Kazakhstan                                 66.0\n"
     ]
    }
   ],
   "source": [
    "top_countries = rank_top_values(opera, group_column='Country Name', value_column='country_change_from_previous_season', top_n=25)\n",
    "print(top_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ff10f9a-ffbd-44c6-907c-e6de178876f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season                                     0\n",
      "iso                                        0\n",
      "city                                       0\n",
      "work                                       5\n",
      "start date                                 0\n",
      "performances                               0\n",
      "Country Name                               0\n",
      "city population                            0\n",
      "country population                         0\n",
      "continent                                  0\n",
      "sub-region                                 0\n",
      "performances_season_by_country             0\n",
      "perf_per_10k_ppl_co_pop                    0\n",
      "performances_season_by_city                0\n",
      "perf_per_1k_ppl_city_pop                   0\n",
      "opera_by_composer                          5\n",
      "performances_season_by_country_total       0\n",
      "performances_season_by_city_total          0\n",
      "perf_total_per_1k_city_pop                 0\n",
      "perf_total_per_10k_co_pop                  0\n",
      "Season Year                                0\n",
      "country_change_from_previous_season     6559\n",
      "city_change_from_previous_season        7191\n",
      "Year                                       0\n",
      "Month                                      0\n",
      "Weekday                                    0\n",
      "Week_of_Year                               0\n",
      "Days_Since_Start                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_count_per_column = opera.isna().sum()\n",
    "print(nan_count_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "243fc173-042a-451b-96a9-f9d8a907e185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season                                     0\n",
      "iso                                        0\n",
      "city                                       0\n",
      "work                                       0\n",
      "start date                                 0\n",
      "performances                               0\n",
      "Country Name                               0\n",
      "city population                            0\n",
      "country population                         0\n",
      "continent                                  0\n",
      "sub-region                                 0\n",
      "performances_season_by_country             0\n",
      "perf_per_10k_ppl_co_pop                    0\n",
      "performances_season_by_city                0\n",
      "perf_per_1k_ppl_city_pop                   0\n",
      "opera_by_composer                          0\n",
      "performances_season_by_country_total       0\n",
      "performances_season_by_city_total          0\n",
      "perf_total_per_1k_city_pop                 0\n",
      "perf_total_per_10k_co_pop                  0\n",
      "Season Year                                0\n",
      "country_change_from_previous_season     6559\n",
      "city_change_from_previous_season        7191\n",
      "Year                                       0\n",
      "Month                                      0\n",
      "Weekday                                    0\n",
      "Week_of_Year                               0\n",
      "Days_Since_Start                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "opera.dropna(subset=['work'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df91dce2-70b5-4864-ad34-b467cf57ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_missing_values(df, target_column, feature_columns, group_column='iso'):\n",
    "    # Step 1: Identify rows with missing values in the target column\n",
    "    missing_data = df[df[target_column].isna()]\n",
    "    \n",
    "    # Step 2: Rows without missing values (training data)\n",
    "    df_no_missing = df.dropna(subset=[target_column])\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X_no_missing = df_no_missing[feature_columns]\n",
    "    y_no_missing = df_no_missing[target_column]\n",
    "    \n",
    "    # Step 3: Train the Linear Regression Model for each group (e.g. 'iso')\n",
    "    predicted_values = []\n",
    "    missing_indices = []\n",
    "\n",
    "    for group_value, group in df_no_missing.groupby(group_column):\n",
    "        X_group = group[feature_columns]\n",
    "        y_group = group[target_column]\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_group, y_group)\n",
    "        \n",
    "        # Get the rows with missing target values in the current group\n",
    "        missing_group = missing_data[missing_data[group_column] == group_value]\n",
    "        \n",
    "        # If there are missing values for this group, predict them\n",
    "        if not missing_group.empty:\n",
    "            X_missing = missing_group[feature_columns]\n",
    "            predictions = model.predict(X_missing)\n",
    "            \n",
    "            # Append the predicted values and their indices\n",
    "            predicted_values.extend(predictions)\n",
    "            missing_indices.extend(missing_group.index)  # Track the indices for replacement\n",
    "\n",
    "    # Step 4: Check if the predicted values match the missing rows\n",
    "    if len(predicted_values) == len(missing_indices):\n",
    "        # Replace the missing values with predictions\n",
    "        df.loc[missing_indices, target_column] = predicted_values\n",
    "    else:\n",
    "        print(f\"Length mismatch: {len(predicted_values)} predicted values, but {len(missing_indices)} missing rows.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74922d78-ba3a-4a16-ab9e-10358884f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   season iso     city                   work start date  performances  \\\n",
      "0    1213  al   Tirana  Ali Pascha von Janina 2013-03-23             4   \n",
      "1    1213  al   Tirana           Don Giovanni 2013-05-18             3   \n",
      "2    1213  al   Tirana                  Tosca 2013-02-13             2   \n",
      "3    1213  am  Yerevan                 Almast 2013-07-11             1   \n",
      "4    1213  am  Yerevan                 Anoush 2013-05-11             3   \n",
      "\n",
      "  Country Name  city population  country population continent  ...  \\\n",
      "0      Albania           418495        2.854907e+06    Europe  ...   \n",
      "1      Albania           418495        2.854907e+06    Europe  ...   \n",
      "2      Albania           418495        2.854907e+06    Europe  ...   \n",
      "3      Armenia          1093485        2.990731e+06      Asia  ...   \n",
      "4      Armenia          1093485        2.990731e+06      Asia  ...   \n",
      "\n",
      "  perf_total_per_1k_city_pop  perf_total_per_10k_co_pop  Season Year  \\\n",
      "0                   0.262847                   0.385301   2013-01-01   \n",
      "1                   0.262847                   0.385301   2013-01-01   \n",
      "2                   0.262847                   0.385301   2013-01-01   \n",
      "3                   0.101510                   0.371147   2013-01-01   \n",
      "4                   0.101510                   0.371147   2013-01-01   \n",
      "\n",
      "   country_change_from_previous_season  city_change_from_previous_season  \\\n",
      "0                             2.230995                          2.230995   \n",
      "1                             2.230995                          2.230995   \n",
      "2                             2.230995                          2.230995   \n",
      "3                             0.000000                          0.000000   \n",
      "4                             0.000000                          0.000000   \n",
      "\n",
      "   Year  Month  Weekday  Week_of_Year  Days_Since_Start  \n",
      "0  2013      3        5            12               273  \n",
      "1  2013      5        5            20               329  \n",
      "2  2013      2        2             7               235  \n",
      "3  2013      7        3            28               383  \n",
      "4  2013      5        5            19               322  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the feature columns\n",
    "feature_columns = ['city population', 'country population', 'performances_season_by_country',\n",
    "                   'perf_per_10k_ppl_co_pop', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year']\n",
    "\n",
    "# Predict missing values for 'country_change_from_previous_season'\n",
    "opera = predict_missing_values(opera, 'country_change_from_previous_season', feature_columns)\n",
    "\n",
    "# Predict missing values for 'city_change_from_previous_season'\n",
    "opera = predict_missing_values(opera, 'city_change_from_previous_season', feature_columns)\n",
    "\n",
    "# Check the result\n",
    "print(opera.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1813f5e-8003-4f6d-a622-b0d1cac94071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>iso</th>\n",
       "      <th>city</th>\n",
       "      <th>work</th>\n",
       "      <th>start date</th>\n",
       "      <th>performances</th>\n",
       "      <th>Country Name</th>\n",
       "      <th>city population</th>\n",
       "      <th>country population</th>\n",
       "      <th>continent</th>\n",
       "      <th>...</th>\n",
       "      <th>Weekday_3</th>\n",
       "      <th>Weekday_4</th>\n",
       "      <th>Weekday_5</th>\n",
       "      <th>Weekday_6</th>\n",
       "      <th>Season Year_2013-01-01 00:00:00</th>\n",
       "      <th>Season Year_2014-01-01 00:00:00</th>\n",
       "      <th>Season Year_2015-01-01 00:00:00</th>\n",
       "      <th>Season Year_2016-01-01 00:00:00</th>\n",
       "      <th>Season Year_2017-01-01 00:00:00</th>\n",
       "      <th>Season Year_2018-01-01 00:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Ali Pascha von Janina</td>\n",
       "      <td>2013-03-23</td>\n",
       "      <td>4</td>\n",
       "      <td>Albania</td>\n",
       "      <td>418495</td>\n",
       "      <td>2.854907e+06</td>\n",
       "      <td>Europe</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Don Giovanni</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>3</td>\n",
       "      <td>Albania</td>\n",
       "      <td>418495</td>\n",
       "      <td>2.854907e+06</td>\n",
       "      <td>Europe</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Tosca</td>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>2</td>\n",
       "      <td>Albania</td>\n",
       "      <td>418495</td>\n",
       "      <td>2.854907e+06</td>\n",
       "      <td>Europe</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1213</td>\n",
       "      <td>am</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Almast</td>\n",
       "      <td>2013-07-11</td>\n",
       "      <td>1</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1093485</td>\n",
       "      <td>2.990731e+06</td>\n",
       "      <td>Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1213</td>\n",
       "      <td>am</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Anoush</td>\n",
       "      <td>2013-05-11</td>\n",
       "      <td>3</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1093485</td>\n",
       "      <td>2.990731e+06</td>\n",
       "      <td>Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season iso     city                   work start date  performances  \\\n",
       "0    1213  al   Tirana  Ali Pascha von Janina 2013-03-23             4   \n",
       "1    1213  al   Tirana           Don Giovanni 2013-05-18             3   \n",
       "2    1213  al   Tirana                  Tosca 2013-02-13             2   \n",
       "3    1213  am  Yerevan                 Almast 2013-07-11             1   \n",
       "4    1213  am  Yerevan                 Anoush 2013-05-11             3   \n",
       "\n",
       "  Country Name  city population  country population continent  ... Weekday_3  \\\n",
       "0      Albania           418495        2.854907e+06    Europe  ...     False   \n",
       "1      Albania           418495        2.854907e+06    Europe  ...     False   \n",
       "2      Albania           418495        2.854907e+06    Europe  ...     False   \n",
       "3      Armenia          1093485        2.990731e+06      Asia  ...      True   \n",
       "4      Armenia          1093485        2.990731e+06      Asia  ...     False   \n",
       "\n",
       "   Weekday_4  Weekday_5  Weekday_6  Season Year_2013-01-01 00:00:00  \\\n",
       "0      False       True      False                             True   \n",
       "1      False       True      False                             True   \n",
       "2      False      False      False                             True   \n",
       "3      False      False      False                             True   \n",
       "4      False       True      False                             True   \n",
       "\n",
       "  Season Year_2014-01-01 00:00:00  Season Year_2015-01-01 00:00:00  \\\n",
       "0                           False                            False   \n",
       "1                           False                            False   \n",
       "2                           False                            False   \n",
       "3                           False                            False   \n",
       "4                           False                            False   \n",
       "\n",
       "   Season Year_2016-01-01 00:00:00  Season Year_2017-01-01 00:00:00  \\\n",
       "0                            False                            False   \n",
       "1                            False                            False   \n",
       "2                            False                            False   \n",
       "3                            False                            False   \n",
       "4                            False                            False   \n",
       "\n",
       "   Season Year_2018-01-01 00:00:00  \n",
       "0                            False  \n",
       "1                            False  \n",
       "2                            False  \n",
       "3                            False  \n",
       "4                            False  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode 'Month', 'Weekday', and 'Day' 'Season Year' and 'opera_by_composer\n",
    "opera = pd.get_dummies(opera, columns=['Month', 'Weekday', 'Season Year'], drop_first=False) \n",
    "\n",
    "# Check the result\n",
    "opera.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52762436-b7f0-4495-af25-82e923e5b809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>iso</th>\n",
       "      <th>city</th>\n",
       "      <th>work</th>\n",
       "      <th>start date</th>\n",
       "      <th>performances</th>\n",
       "      <th>Country Name</th>\n",
       "      <th>city population</th>\n",
       "      <th>country population</th>\n",
       "      <th>continent</th>\n",
       "      <th>...</th>\n",
       "      <th>Weekday_3</th>\n",
       "      <th>Weekday_4</th>\n",
       "      <th>Weekday_5</th>\n",
       "      <th>Weekday_6</th>\n",
       "      <th>Season Year_2013-01-01 00:00:00</th>\n",
       "      <th>Season Year_2014-01-01 00:00:00</th>\n",
       "      <th>Season Year_2015-01-01 00:00:00</th>\n",
       "      <th>Season Year_2016-01-01 00:00:00</th>\n",
       "      <th>Season Year_2017-01-01 00:00:00</th>\n",
       "      <th>Season Year_2018-01-01 00:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Ali Pascha von Janina</td>\n",
       "      <td>2013-03-23</td>\n",
       "      <td>4</td>\n",
       "      <td>Albania</td>\n",
       "      <td>418495</td>\n",
       "      <td>2.854907e+06</td>\n",
       "      <td>Europe</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Don Giovanni</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>3</td>\n",
       "      <td>Albania</td>\n",
       "      <td>418495</td>\n",
       "      <td>2.854907e+06</td>\n",
       "      <td>Europe</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1213</td>\n",
       "      <td>al</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Tosca</td>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>2</td>\n",
       "      <td>Albania</td>\n",
       "      <td>418495</td>\n",
       "      <td>2.854907e+06</td>\n",
       "      <td>Europe</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1213</td>\n",
       "      <td>am</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Almast</td>\n",
       "      <td>2013-07-11</td>\n",
       "      <td>1</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1093485</td>\n",
       "      <td>2.990731e+06</td>\n",
       "      <td>Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1213</td>\n",
       "      <td>am</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Anoush</td>\n",
       "      <td>2013-05-11</td>\n",
       "      <td>3</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1093485</td>\n",
       "      <td>2.990731e+06</td>\n",
       "      <td>Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season iso     city                   work start date  performances  \\\n",
       "0    1213  al   Tirana  Ali Pascha von Janina 2013-03-23             4   \n",
       "1    1213  al   Tirana           Don Giovanni 2013-05-18             3   \n",
       "2    1213  al   Tirana                  Tosca 2013-02-13             2   \n",
       "3    1213  am  Yerevan                 Almast 2013-07-11             1   \n",
       "4    1213  am  Yerevan                 Anoush 2013-05-11             3   \n",
       "\n",
       "  Country Name  city population  country population continent  ... Weekday_3  \\\n",
       "0      Albania           418495        2.854907e+06    Europe  ...         0   \n",
       "1      Albania           418495        2.854907e+06    Europe  ...         0   \n",
       "2      Albania           418495        2.854907e+06    Europe  ...         0   \n",
       "3      Armenia          1093485        2.990731e+06      Asia  ...         1   \n",
       "4      Armenia          1093485        2.990731e+06      Asia  ...         0   \n",
       "\n",
       "   Weekday_4  Weekday_5  Weekday_6  Season Year_2013-01-01 00:00:00  \\\n",
       "0          0          1          0                                1   \n",
       "1          0          1          0                                1   \n",
       "2          0          0          0                                1   \n",
       "3          0          0          0                                1   \n",
       "4          0          1          0                                1   \n",
       "\n",
       "  Season Year_2014-01-01 00:00:00  Season Year_2015-01-01 00:00:00  \\\n",
       "0                               0                                0   \n",
       "1                               0                                0   \n",
       "2                               0                                0   \n",
       "3                               0                                0   \n",
       "4                               0                                0   \n",
       "\n",
       "   Season Year_2016-01-01 00:00:00  Season Year_2017-01-01 00:00:00  \\\n",
       "0                                0                                0   \n",
       "1                                0                                0   \n",
       "2                                0                                0   \n",
       "3                                0                                0   \n",
       "4                                0                                0   \n",
       "\n",
       "   Season Year_2018-01-01 00:00:00  \n",
       "0                                0  \n",
       "1                                0  \n",
       "2                                0  \n",
       "3                                0  \n",
       "4                                0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creation_source dummies are showing as boolean, switch to integer using the function below\n",
    "def convert_booleans_to_integers(df):\n",
    "    # Loop through columns and convert those with boolean dtype to integers\n",
    "    for column in df.select_dtypes(include='bool').columns:\n",
    "        df[column] = df[column].astype(int)\n",
    "    return df\n",
    "\n",
    "# Use new function\n",
    "opera = convert_booleans_to_integers(opera)\n",
    "\n",
    "# Check the first few rows to see the change\n",
    "opera.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1119eb6-6a5b-4a72-99ed-8497cdbfb354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season                                  0\n",
      "iso                                     0\n",
      "city                                    0\n",
      "work                                    0\n",
      "start date                              0\n",
      "performances                            0\n",
      "Country Name                            0\n",
      "city population                         0\n",
      "country population                      0\n",
      "continent                               0\n",
      "sub-region                              0\n",
      "performances_season_by_country          0\n",
      "perf_per_10k_ppl_co_pop                 0\n",
      "performances_season_by_city             0\n",
      "perf_per_1k_ppl_city_pop                0\n",
      "opera_by_composer                       0\n",
      "performances_season_by_country_total    0\n",
      "performances_season_by_city_total       0\n",
      "perf_total_per_1k_city_pop              0\n",
      "perf_total_per_10k_co_pop               0\n",
      "country_change_from_previous_season     0\n",
      "city_change_from_previous_season        0\n",
      "Year                                    0\n",
      "Week_of_Year                            0\n",
      "Days_Since_Start                        0\n",
      "Month_1                                 0\n",
      "Month_2                                 0\n",
      "Month_3                                 0\n",
      "Month_4                                 0\n",
      "Month_5                                 0\n",
      "Month_6                                 0\n",
      "Month_7                                 0\n",
      "Month_8                                 0\n",
      "Month_9                                 0\n",
      "Month_10                                0\n",
      "Month_11                                0\n",
      "Month_12                                0\n",
      "Weekday_0                               0\n",
      "Weekday_1                               0\n",
      "Weekday_2                               0\n",
      "Weekday_3                               0\n",
      "Weekday_4                               0\n",
      "Weekday_5                               0\n",
      "Weekday_6                               0\n",
      "Season Year_2013-01-01 00:00:00         0\n",
      "Season Year_2014-01-01 00:00:00         0\n",
      "Season Year_2015-01-01 00:00:00         0\n",
      "Season Year_2016-01-01 00:00:00         0\n",
      "Season Year_2017-01-01 00:00:00         0\n",
      "Season Year_2018-01-01 00:00:00         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "opera.dropna(subset=['country_change_from_previous_season'], inplace=True)\n",
    "print(opera.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84db532b-6de1-448e-8cfc-0325e6b44bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "#function to evaluate linear regression\n",
    "def evaluate_linear_regression(model, X, y_columns, sub_region_groups, scaler):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a linear regression model for each sub-region,\n",
    "    then returns the average MAE, RMSE, and R² score for each target column,\n",
    "    and also stores predictions for later sorting.\n",
    "    \"\"\"\n",
    "    metrics = {}  # Dictionary to store metrics for each target column\n",
    "    all_predictions = []  # List to store all predictions for sorting\n",
    "\n",
    "    for target_column in y_columns:\n",
    "        mae_scores, rmse_scores, r2_scores = [], [], []\n",
    "\n",
    "        for country_or_city, group in sub_region_groups:\n",
    "            if len(group) > 1:\n",
    "                # Extract features and target\n",
    "                X_group = group[X.columns]\n",
    "                y_group = group[target_column]\n",
    "\n",
    "                # Train-test split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X_group, y_group, test_size=0.2, random_state=42)\n",
    "\n",
    "                # Scale the data\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "                # Train the model\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "\n",
    "                # Make predictions\n",
    "                predictions = model.predict(X_test_scaled)\n",
    "\n",
    "                # Store predictions for sorting\n",
    "                for idx, pred in enumerate(predictions):\n",
    "                    all_predictions.append({\n",
    "                        'Country Name': group['Country Name'].iloc[idx],  # or 'sub-region' if applicable\n",
    "                        'Predicted Growth': pred\n",
    "                    })\n",
    "\n",
    "                # Compute evaluation metrics\n",
    "                mae_scores.append(mean_absolute_error(y_test, predictions))\n",
    "                rmse_scores.append(np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "                r2_scores.append(r2_score(y_test, predictions))\n",
    "\n",
    "        # Compute average metrics for the current target column\n",
    "        avg_metrics = {\n",
    "            'MAE': np.mean(mae_scores) if mae_scores else 0,\n",
    "            'RMSE': np.mean(rmse_scores) if rmse_scores else 0,\n",
    "            'R2': np.mean(r2_scores) if r2_scores else 0\n",
    "        }\n",
    "\n",
    "        # Print each metric on a separate line\n",
    "        print(f\"Metrics for {target_column}:\")\n",
    "        print(f\"  MAE: {avg_metrics['MAE']}\")\n",
    "        print(f\"  RMSE: {avg_metrics['RMSE']}\")\n",
    "        print(f\"  R2: {avg_metrics['R2']}\")\n",
    "\n",
    "        # Store metrics for this target column\n",
    "        metrics[target_column] = avg_metrics\n",
    "\n",
    "    # Sort predictions based on predicted growth (descending order)\n",
    "    predictions_df = pd.DataFrame(all_predictions)\n",
    "    predictions_df = predictions_df.sort_values(by='Predicted Growth', ascending=False)\n",
    "\n",
    "    # Drop duplicates, keeping the row with the highest predicted growth for each country\n",
    "    predictions_df_unique = predictions_df.drop_duplicates(subset='Country Name', keep='first')\n",
    "\n",
    "    return metrics, predictions_df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb31b7a0-1f63-4bec-9602-0e4a289d7f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Evaluation Results:\n",
      "{'country_change_from_previous_season': {'MAE': 39.30978638759204, 'RMSE': 52.203724902632906, 'R2': -14.181765868919383}}\n",
      "            Country Name  Predicted Growth\n",
      "1936              Brazil        575.052553\n",
      "1974            Colombia        575.052553\n",
      "1911           Argentina        575.052553\n",
      "1972               Chile        514.740053\n",
      "1995              Mexico        394.552553\n",
      "3282       United States        355.075120\n",
      "3450              Canada        351.075120\n",
      "1988          Costa Rica        343.552553\n",
      "324             Bulgaria        333.164941\n",
      "1292  Russian Federation        330.789941\n"
     ]
    }
   ],
   "source": [
    "# List of target columns (for country prediction)\n",
    "target_columns = ['country_change_from_previous_season']\n",
    "\n",
    "# Call the function to evaluate the model for 'country_change_from_previous_season'\n",
    "evaluation_results_country, predictions_df_country = evaluate_linear_regression(\n",
    "    model, opera[feature_columns], target_columns, sub_region_groups, scaler)\n",
    "\n",
    "# Print out the evaluation results for country\n",
    "print(\"Country Evaluation Results:\")\n",
    "print(evaluation_results_country)\n",
    "\n",
    "# Get top 50 countries with highest predicted growth\n",
    "top_countries_predicted = predictions_df_country.head(50)\n",
    "\n",
    "# Print the top 10 predicted countries\n",
    "print(top_countries_predicted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ec1b3-f043-4ec6-99aa-80a8d1800325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target columns (for country prediction)\n",
    "target_columns = ['country_change_from_previous_season']\n",
    "\n",
    "# Call the function to evaluate the model for 'country_change_from_previous_season'\n",
    "evaluation_results_country, predictions_df_country = evaluate_linear_regression(\n",
    "    model, opera[feature_columns], target_columns, sub_region_groups, scaler)\n",
    "\n",
    "# Print out the evaluation results for country\n",
    "print(\"Country Evaluation Results:\")\n",
    "print(evaluation_results_country)\n",
    "\n",
    "# Get top 50 countries with highest predicted growth\n",
    "top_countries_predicted = predictions_df_country.head(50)\n",
    "\n",
    "# Print the top 10 predicted countries\n",
    "print(top_countries_predicted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f090f37-06ec-4272-8422-605730ecdde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Country Name  predicted_country_growth\n",
      "263      Australia                 75.180739\n",
      "190        Austria                 72.237065\n",
      "2427       Finland                 63.241543\n",
      "3883       Romania                 59.168818\n",
      "4515      Slovenia                 58.507863\n",
      "16338      Hungary                 58.266885\n",
      "3390     Lithuania                 58.138222\n",
      "3798        Poland                 56.789262\n",
      "4615        Turkey                 47.124328\n",
      "14132  Switzerland                 46.597059\n"
     ]
    }
   ],
   "source": [
    "X_country = opera[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
    "                   'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                   'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "\n",
    "# Dependent variable (growth at country level)\n",
    "y_country = opera['country_change_from_previous_season']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_country, y_test_country = train_test_split(X_country, y_country, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data (using the same scaler as the training data)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model for country growth prediction\n",
    "lin_country = LinearRegression()\n",
    "\n",
    "# Train the model on scaled data\n",
    "lin_country.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "# Transform the entire country data to scaled values before prediction (using the same scaler)\n",
    "X_country_scaled = scaler.transform(X_country)\n",
    "\n",
    "# Predict growth for the entire country dataset\n",
    "opera['predicted_country_growth'] = lin_country.predict(X_country_scaled)\n",
    "\n",
    "# Sort countries based on predicted growth\n",
    "df_country_growth_sorted = opera[['Country Name', 'predicted_country_growth']].sort_values(by='predicted_country_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each city\n",
    "df_opera_country_growth_unique = df_country_growth_sorted.drop_duplicates(subset='Country Name', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_countries_predicted = df_opera_country_growth_unique.head(10)\n",
    "print(top_countries_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3979f988-f03f-4459-90e3-0e3c6fa06c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 147.3972739647199\n",
      "MSE: 41254.4963747648\n",
      "RMSE: 203.1120291237444\n",
      "R2_Score: 0.038655174243710566\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "y_pred_country_lin = lin_country.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "lin_evaluate_model(y_test_country, y_pred_country_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ea2c806-91e9-48e7-879e-851a02cd9d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      iso         city  predicted_city_growth\n",
      "263    au       Sydney              75.180739\n",
      "190    at       Vienna              72.237065\n",
      "2427   fi     Helsinki              63.241543\n",
      "3883   ro  Cluj-Napoca              59.168818\n",
      "4515   si      Maribor              58.507863\n",
      "16338  hu     Budapest              58.266885\n",
      "3390   lt      Vilnius              58.138222\n",
      "3798   pl      Wroclaw              56.789262\n",
      "10736  ro    Timisoara              55.113396\n",
      "10113  lt       Kaunas              53.492196\n"
     ]
    }
   ],
   "source": [
    "# Independent variables (predictors)\n",
    "X_city = opera[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
    "                   'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                   'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "\n",
    "# Dependent variable (growth at city level)\n",
    "y_city = opera['city_change_from_previous_season']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_city, y_test_city = train_test_split(X_city, y_city, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data (using the same scaler as the training data)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model for city growth prediction\n",
    "lin_city = LinearRegression()\n",
    "\n",
    "# Train the model on scaled data\n",
    "lin_city.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "# Transform the entire city data to scaled values before prediction (using the same scaler)\n",
    "X_city_scaled = scaler.transform(X_city)\n",
    "\n",
    "# Predict growth for the entire city dataset\n",
    "opera['predicted_city_growth'] = lin_country.predict(X_city_scaled)\n",
    "\n",
    "# Sort cities based on predicted growth\n",
    "df_city_growth_sorted = opera[['iso', 'city', 'predicted_city_growth']].sort_values(by='predicted_city_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each city\n",
    "df_opera_city_growth_unique = df_city_growth_sorted.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "# Get top 25 cities with highest predicted growth\n",
    "top_cities_predicted = df_opera_city_growth_unique.head(10)\n",
    "print(top_cities_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9df4f37f-de92-47d0-8d7c-6a3b7af80920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 20.428280777427656\n",
      "MSE: 5380.81251451332\n",
      "RMSE: 73.35402180189796\n",
      "R2_Score: 0.0036593842846965874\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "y_pred_city_lin = lin_city.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "lin_evaluate_model(y_test_city, y_pred_city_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077dd158-9c80-4238-9d4d-021df831b682",
   "metadata": {},
   "source": [
    "Linear Regression and grouping by sub-region for predictive analysis. This time I grouped the opera data by sub-regions before peforming the linear regression to see if it improved predictions.  The scores were significantly better for the cities prediction but they still were underperforming for the country data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07535be7-df93-404a-b594-d67b6fd5a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Country Name  predicted_country_growth\n",
      "12366       United States                384.528795\n",
      "16879          Kazakhstan                353.238890\n",
      "10028          Kyrgyzstan                325.570095\n",
      "32851          Uzbekistan                270.377483\n",
      "30634  Russian Federation                259.203609\n",
      "33363              Canada                248.547297\n",
      "17247              Poland                180.091249\n",
      "3100                Italy                 92.900284\n",
      "15701               Egypt                 90.607635\n",
      "3917              Romania                 77.953992\n"
     ]
    }
   ],
   "source": [
    "# Independent variables (predictors)\n",
    "X_country = opera[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
    "                   'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                   'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "\n",
    "# Dependent variable (growth at country level)\n",
    "y_country = opera['country_change_from_previous_season']\n",
    "\n",
    "# Group by sub-region\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Model for country growth prediction\n",
    "lin_country = LinearRegression()\n",
    "\n",
    "# Store the predicted values in a DataFrame\n",
    "predicted_growth_values = pd.Series(index=opera.index)  # Create an empty Series to store predictions\n",
    "\n",
    "# Loop through each sub-region group to train and predict separately\n",
    "for country_or_city, group in sub_region_groups:\n",
    "    # If the group has more than one sample, split and train\n",
    "    if len(group) > 1:\n",
    "        # Extract the features and target for this country group\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['country_change_from_previous_season']\n",
    "\n",
    "        # Split data into train and test sets (80% train, 20% test)\n",
    "        X_train, X_test, y_train_country, y_test_country = train_test_split(X_group, y_group, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # Train the model on the training data\n",
    "        lin_country.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "        # Scale the test data using the same scaler\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Make predictions for the test set\n",
    "        predictions = lin_country.predict(X_test_scaled)\n",
    "\n",
    "        # Assign predictions to the correct indices in the original DataFrame\n",
    "        predicted_growth_values.loc[X_test.index] = predictions  # Correctly assign predictions to test set rows\n",
    "    else:\n",
    "        # If the group has only one sample, predict directly and append\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['country_change_from_previous_season']\n",
    "        \n",
    "        # Scale the data for this group\n",
    "        X_scaled = scaler.fit_transform(X_group)\n",
    "\n",
    "        # Train the model\n",
    "        lin_country.fit(X_scaled, y_group)\n",
    "\n",
    "        # Make predictions for this group\n",
    "        predictions = lin_country.predict(X_scaled)\n",
    "        \n",
    "        # Assign predictions to the correct indices in the original DataFrame\n",
    "        predicted_growth_values.loc[group.index] = predictions  # Correctly assign predictions to the original rows\n",
    "\n",
    "# Assign the predicted values to the original DataFrame\n",
    "opera['predicted_country_growth'] = predicted_growth_values\n",
    "\n",
    "# Sort countries based on predicted growth\n",
    "df_country_growth_sorted = opera[['Country Name', 'predicted_country_growth']].sort_values(by='predicted_country_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each country\n",
    "df_opera_country_growth_unique = df_country_growth_sorted.drop_duplicates(subset='Country Name', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_countries_predicted_lin = df_opera_country_growth_unique.head(10)\n",
    "print(top_countries_predicted_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "958e9637-d052-43af-a8b6-bcd08e1440ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average Linear Regression Metrics Across Sub-Regions for Countries:\n",
      "   MAE: 62.5307\n",
      "   RMSE: 103.0777\n",
      "   R2: -1.5042\n"
     ]
    }
   ],
   "source": [
    "lin_metrics_country = evaluate_linear_regression(lin_country, X_country, y_country, sub_region_groups, scaler)\n",
    "\n",
    "print(\"\\n🔹 Average Linear Regression Metrics Across Sub-Regions for Countries:\")\n",
    "for metric, value in lin_metrics_country.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d23f7fad-c708-4366-a093-95340f84c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      iso           city  predicted_city_growth\n",
      "15701  eg     Alexandria              71.954844\n",
      "30467  ru         Moscow              52.171187\n",
      "26239  us     Washington              51.766670\n",
      "10389  nz     Berhampore              42.547934\n",
      "13054  us    Saint Louis              41.165920\n",
      "11188  ru  St Petersburg              38.523518\n",
      "11980  us      Arlington              34.697543\n",
      "6536   am        Yerevan              34.630684\n",
      "12661  us        Madison              32.847327\n",
      "12412  us        Houston              32.374409\n"
     ]
    }
   ],
   "source": [
    "# Independent variables (predictors)\n",
    "X_city = opera[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
    "                   'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                   'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "\n",
    "# Dependent variable (growth at city level)\n",
    "y_city = opera['city_change_from_previous_season']\n",
    "\n",
    "# Still group the cities by sub-region\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Model for country growth prediction\n",
    "lin_city = LinearRegression()\n",
    "\n",
    "# Store the predicted values in a DataFrame\n",
    "predicted_growth_values = pd.Series(index=opera.index)  # Create an empty Series to store predictions\n",
    "\n",
    "# Loop through each country group to train and predict separately\n",
    "for country_or_city, group in sub_region_groups:\n",
    "    # If the group has more than one sample, split and train\n",
    "    if len(group) > 1:\n",
    "        # Extract the features and target for this country group\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['city_change_from_previous_season']\n",
    "\n",
    "        # Split data into train and test sets (80% train, 20% test)\n",
    "        X_train, X_test, y_train_city, y_test_city = train_test_split(X_group, y_group, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # Train the model on the training data\n",
    "        lin_city.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "        # Scale the test data using the same scaler\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Make predictions for the test set\n",
    "        predictions = lin_city.predict(X_test_scaled)\n",
    "\n",
    "        # Assign predictions to the correct indices in the original DataFrame\n",
    "        predicted_growth_values.loc[X_test.index] = predictions  # Correctly assign predictions to test set rows\n",
    "    else:\n",
    "        # If the group has only one sample, predict directly and append\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['city_change_from_previous_season']\n",
    "        \n",
    "        # Scale the data for this group\n",
    "        X_scaled = scaler.fit_transform(X_group)\n",
    "\n",
    "        # Train the model\n",
    "        lin_city.fit(X_scaled, y_group)\n",
    "\n",
    "        # Make predictions for this group\n",
    "        predictions = lin_city.predict(X_scaled)\n",
    "        \n",
    "        # Assign predictions to the correct indices in the original DataFrame\n",
    "        predicted_growth_values.loc[group.index] = predictions  # Correctly assign predictions to the original rows\n",
    "\n",
    "# Assign the predicted values to the original DataFrame\n",
    "opera['predicted_city_growth'] = predicted_growth_values\n",
    "\n",
    "# Sort countries based on predicted growth\n",
    "df_city_growth_sorted = opera[['iso', 'city', 'predicted_city_growth']].sort_values(by='predicted_city_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each country\n",
    "df_opera_city_growth_unique = df_city_growth_sorted.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_cities_predicted_lin = df_opera_city_growth_unique.head(10)\n",
    "print(top_cities_predicted_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7673ad5-e24a-4277-a639-86068f1db99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average Linear Regression Metrics Across Sub-Regions for Cities:\n",
      "   MAE: 12.7954\n",
      "   RMSE: 19.0671\n",
      "   R2: -4.0879\n"
     ]
    }
   ],
   "source": [
    "lin_metrics_city = evaluate_linear_regression(lin_city, X_city, y_city, sub_region_groups, scaler)\n",
    "\n",
    "print(\"\\n🔹 Average Linear Regression Metrics Across Sub-Regions for Cities:\")\n",
    "for metric, value in lin_metrics_city.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbe008-a80f-4c5b-afe4-6a8762e98a91",
   "metadata": {},
   "source": [
    "Logistic Regression Models - let's see if we can get a better predictive model using Logistic regression without countries being grouped first and then try it with grouping countries. In order to run a Logistic regression model, I binned the country_change_from_previous_season by quartiles into categories of Low, Medium, High and Very High so there is a category that is being used as the dependent outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "047ba9a7-82f8-4ab6-8e5d-fb8257fc0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this function to evaluate performance of models\n",
    "def evaluate_model_performance(model, X, y, sub_region_groups, scaler, bins, labels):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model for each sub-region, then returns the average accuracy, precision, and recall.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to train (e.g., RandomForestClassifier, LogisticRegression).\n",
    "    - X: Feature DataFrame.\n",
    "    - y: Target variable DataFrame.\n",
    "    - country_groups: Grouped data by sub-region.\n",
    "    - scaler: StandardScaler instance.\n",
    "    - bins: Bins for categorizing target values.\n",
    "    - labels: Labels corresponding to bins.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with average accuracy, precision, and recall.\n",
    "    \"\"\"\n",
    "    accuracies, precisions, recalls = [], [], []\n",
    "\n",
    "    for sub_region, group in sub_region_groups:\n",
    "        if len(group) > 1 and len(group['country_change_from_previous_season'].unique()) > 1:\n",
    "            # Extract features and target\n",
    "            X_group = group[X.columns]\n",
    "            y_group = group[y.name]\n",
    "\n",
    "            # Convert the target variable to categorical labels\n",
    "            y_group_classified = pd.cut(y_group, bins=bins, labels=labels)\n",
    "\n",
    "            # Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Scale the training and test data\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Train only if there are multiple classes\n",
    "            if len(y_train.unique()) > 1:\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                predictions = model.predict(X_test_scaled)\n",
    "\n",
    "                # Store evaluation metrics\n",
    "                accuracies.append(accuracy_score(y_test, predictions))\n",
    "                precisions.append(precision_score(y_test, predictions, average='weighted', zero_division=0))\n",
    "                recalls.append(recall_score(y_test, predictions, average='weighted', zero_division=0))\n",
    "\n",
    "    # Compute the average metrics\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean(accuracies) if accuracies else 0,\n",
    "        'precision': np.mean(precisions) if precisions else 0,\n",
    "        'recall': np.mean(recalls) if recalls else 0\n",
    "    }\n",
    "\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2120524-767c-4f77-8f52-e3af6cfe4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Independent variables (predictors)\n",
    "X_country = opera[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
    "                   'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                   'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "\n",
    "# Dependent variable (growth at country level)\n",
    "y_country = opera['country_change_from_previous_season']\n",
    "\n",
    "# Convert continuous growth to categorical labels (Low, Medium, High)\n",
    "q1 = opera['country_change_from_previous_season'].quantile(0.25)\n",
    "q2 = opera['country_change_from_previous_season'].quantile(0.50)\n",
    "q3 = opera['country_change_from_previous_season'].quantile(0.75)\n",
    "\n",
    "bins = [-float('inf'), q1, q2, q3, float('inf')]\n",
    "labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "y_country_classified = pd.cut(y_country, bins=bins, labels=labels)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_country, y_test_country = train_test_split(X_country, y_country_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data (using the same scaler as the training data)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e399022d-e0dc-47ca-a7e2-48909fe1a998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'solver': 'saga'}\n",
      "Best F1 Score: 0.48709776078392775\n"
     ]
    }
   ],
   "source": [
    "# Initialize LogisticRegression\n",
    "lr_country = LogisticRegression(max_iter=10000, multi_class='ovr')\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # Use 'liblinear' and 'saga' or 'lbfgs' for L1\n",
    "}\n",
    "\n",
    "# Define the custom scoring function (e.g., F1 score)\n",
    "custom_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# GridSearchCV with custom scoring\n",
    "grid_search = GridSearchCV(estimator=lr_country, param_grid=param_grid, scoring=custom_scorer, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "# Output the best parameters and the best F1 score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e8812b3-16fb-4201-be26-ccdbe5eeb4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the sub-region: Northern Africa due to insufficient class variation in the target.\n",
      "Skipping the sub-region: South-eastern Asia due to insufficient class variation in the target.\n",
      "             Country Name predicted_country_growth\n",
      "14846             Germany                Very High\n",
      "13313       United States                Very High\n",
      "35890               Italy                Very High\n",
      "30564  Russian Federation                Very High\n",
      "11596      United Kingdom                Very High\n",
      "9024                Spain                Very High\n",
      "18084              Sweden                Very High\n",
      "35573             Hungary                   Medium\n",
      "27086      Czech Republic                   Medium\n",
      "15895              France                   Medium\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found during the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create the Logistic Regression model with the best parameters\n",
    "lr_country = LogisticRegression(\n",
    "    C=best_params['C'],\n",
    "    solver=best_params['solver'],\n",
    "    max_iter=10000,\n",
    "    multi_class='ovr'  # Specify multiclass handling (One-vs-Rest)\n",
    ")\n",
    "\n",
    "# Initialize a container to store predictions with correct dtype (categorical or object)\n",
    "predicted_growth_values = pd.Series(index=opera.index, dtype='object')\n",
    "\n",
    "# Group the data by 'Country Name'\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Loop through each country group to train and predict separately\n",
    "for sub_region, group in sub_region_groups:\n",
    "    # If the group has more than one sample and more than one class, split and train\n",
    "    if len(group) > 1 and len(group['country_change_from_previous_season'].unique()) > 1:\n",
    "        # Extract the features and target for this country group\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['country_change_from_previous_season']\n",
    "\n",
    "        # Convert the target to categorical labels (same as before)\n",
    "        y_group_classified = pd.cut(y_group, bins=bins, labels=labels)\n",
    "\n",
    "        # Split data into train and test sets (80% train, 20% test)\n",
    "        X_train, X_test, y_train_country, y_test_country = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # **Only train if the target variable has more than one class**\n",
    "        if len(y_train_country.unique()) > 1:\n",
    "            # Train the model on the training data\n",
    "            lr_country.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "            # Scale the test data using the same scaler\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Make predictions for the test set\n",
    "            predictions = lr_country.predict(X_test_scaled)\n",
    "\n",
    "            # Assign predictions to the correct indices in the original DataFrame\n",
    "            predicted_growth_values.loc[X_test.index] = predictions  # Correctly assign predictions to test set rows\n",
    "        else:\n",
    "            print(f\"Skipping the sub-region: {sub_region} due to insufficient class variation in the target.\")\n",
    "            continue\n",
    "    else:\n",
    "        # Skip the group if it has only one class or one sample\n",
    "        print(f\"Skipping the sub-region: {sub_region} due to insufficient class variation or sample size.\")\n",
    "        continue\n",
    "\n",
    "# You can now inspect or save the results\n",
    "opera['predicted_country_growth'] = predicted_growth_values\n",
    "\n",
    "# Sort and get the top countries based on predicted growth\n",
    "df_country_growth_sorted = opera[['Country Name', 'predicted_country_growth']].sort_values(by='predicted_country_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each country\n",
    "df_opera_country_growth_unique = df_country_growth_sorted.drop_duplicates(subset='Country Name', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_countries_predicted_lr1 = df_opera_country_growth_unique.head(10)\n",
    "print(top_countries_predicted_lr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84012119-4125-4c3b-a6f2-9736880b3113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average Logistic Regression Metrics Across Sub-Regions for Countries using GridSearch:\n",
      "   accuracy: 0.8292\n",
      "   precision: 0.8242\n",
      "   recall: 0.8292\n"
     ]
    }
   ],
   "source": [
    "lr_metrics_country1 = evaluate_model_performance(lr_country, X_country, y_country, sub_region_groups, scaler, bins, labels)\n",
    "\n",
    "print(f\"\\n🔹 Average Logistic Regression Metrics Across Sub-Regions for Countries using GridSearch:\")\n",
    "for metric, value in lr_metrics_country1.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5001e3ba-3289-47b7-b002-c8d70e31c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables (predictors)\n",
    "X_city = opera[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
    "                   'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                   'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "\n",
    "# Dependent variable (growth at city level)\n",
    "y_city = opera['city_change_from_previous_season']\n",
    "\n",
    "# Convert continuous growth to categorical labels (Low, Medium, High)\n",
    "q1 = opera['city_change_from_previous_season'].quantile(0.25)\n",
    "q2 = opera['city_change_from_previous_season'].quantile(0.50)\n",
    "q3 = opera['city_change_from_previous_season'].quantile(0.75)\n",
    "\n",
    "bins = [-float('inf'), q1, q2, q3, float('inf')]\n",
    "labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "y_city_classified = pd.cut(y_city, bins=bins, labels=labels)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_city, y_test_city = train_test_split(X_city, y_city_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data (using the same scaler as the training data)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c56e026-de24-46b0-899f-c6950b4c0cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'solver': 'liblinear'}\n",
      "Best F1 Score: 0.3248013022646522\n"
     ]
    }
   ],
   "source": [
    "# Initialize LogisticRegression\n",
    "lr_city = LogisticRegression(max_iter=10000, multi_class='ovr')\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # Use 'liblinear' and 'saga' or 'lbfgs' for L1\n",
    "}\n",
    "\n",
    "# Define the custom scoring function (e.g., F1 score)\n",
    "custom_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# GridSearchCV with custom scoring\n",
    "grid_search = GridSearchCV(estimator=lr_city, param_grid=param_grid, scoring=custom_scorer, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "# Output the best parameters and the best F1 score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aedc5821-74c8-40bc-8138-726abc2231d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      iso          city predicted_city_growth\n",
      "38935  uz      Tashkent             Very High\n",
      "32944  at          Graz             Very High\n",
      "33004  at      Salzburg             Very High\n",
      "32984  at          Linz             Very High\n",
      "32959  at     Innsbruck             Very High\n",
      "13343  us    Washington             Very High\n",
      "32898  ar  Buenos Aires             Very High\n",
      "32868  za     Cape Town             Very High\n",
      "32839  us    Wilmington             Very High\n",
      "33126  au     Melbourne             Very High\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found during the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create the Logistic Regression model with the best parameters\n",
    "lr_city = LogisticRegression(\n",
    "    C=best_params['C'],\n",
    "    solver=best_params['solver'],\n",
    "    max_iter=10000,\n",
    "    multi_class='ovr'  # Specify multiclass handling (One-vs-Rest)\n",
    ")\n",
    "\n",
    "# Initialize a container to store predictions with correct dtype (categorical or object)\n",
    "predicted_growth_values = pd.Series(index=opera.index, dtype='object')\n",
    "\n",
    "# Still group the data by 'sub-region'\n",
    "sub_region = opera.groupby('sub-region')\n",
    "\n",
    "# Loop through each sub-region group to train and predict separately for cities\n",
    "for sub_region, group in sub_region_groups:\n",
    "    # If the group has more than one sample and more than one class, split and train\n",
    "    if len(group) > 1 and len(group['city_change_from_previous_season'].unique()) > 1:\n",
    "        # Extract the features and target for this sub-region group\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['city_change_from_previous_season']\n",
    "\n",
    "        # Convert the target to categorical labels (same as before)\n",
    "        y_group_classified = pd.cut(y_group, bins=bins, labels=labels)\n",
    "\n",
    "        # Split data into train and test sets (80% train, 20% test)\n",
    "        X_train, X_test, y_train_city, y_test_city = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # **Only train if the target variable has more than one class**\n",
    "        if len(y_train_city.unique()) > 1:\n",
    "            # Train the model on the training data\n",
    "            lr_city.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "            # Scale the test data using the same scaler\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Make predictions for the test set\n",
    "            predictions = lr_city.predict(X_test_scaled)\n",
    "\n",
    "            # Ensure the indices of predictions and X_test match\n",
    "            predicted_growth_values.loc[X_test.index] = predictions  # Correctly assign predictions to test set rows\n",
    "        else:\n",
    "            print(f\"Skipping sub-region: {sub_region} due to insufficient class variation in the target.\")\n",
    "            continue\n",
    "    else:\n",
    "        # Skip the group if it has only one class or one sample\n",
    "        print(f\"Skipping sub-region: {sub_region} due to insufficient class variation or sample size.\")\n",
    "        continue\n",
    "\n",
    "# You can now inspect or save the results\n",
    "opera['predicted_city_growth'] = predicted_growth_values\n",
    "\n",
    "# Sort and get the top cities based on predicted growth\n",
    "df_city_growth_sorted = opera[['iso', 'city', 'predicted_city_growth']].sort_values(by='predicted_city_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each city\n",
    "df_opera_city_growth_unique = df_city_growth_sorted.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "# Get top 10 cities with highest predicted growth\n",
    "top_cities_predicted_lr1 = df_opera_city_growth_unique.head(10)\n",
    "print(top_cities_predicted_lr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "257fefb0-50f9-40fd-9507-0c40e136c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average Logistic Regression Metrics Across Sub-Regions for Cities using GridSearch:\n",
      "   accuracy: 0.5203\n",
      "   precision: 0.5411\n",
      "   recall: 0.5203\n"
     ]
    }
   ],
   "source": [
    "lr_metrics_city1 = evaluate_model_performance(lr_city, X_city, y_city, sub_region_groups, scaler, bins, labels)\n",
    "\n",
    "print(f\"\\n🔹 Average Logistic Regression Metrics Across Sub-Regions for Cities using GridSearch:\")\n",
    "for metric, value in lr_metrics_city1.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15bcacd-3021-4698-8357-eb8f1285dbe6",
   "metadata": {},
   "source": [
    "Let's see if there's better results using RandomizedSearchCV instead of GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0f6e7b35-287f-4c71-9958-765b4108068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 42 is smaller than n_iter=100. Running 42 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'solver': 'liblinear', 'max_iter': 1000, 'C': 1000.0}\n",
      "Best F1 Score: 0.6786559537318885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Initialize LogisticRegression\n",
    "lr_country = LogisticRegression(max_iter=20000, multi_class='ovr')\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 7),  # Exponential range for C (e.g., 0.001, 0.01, 0.1, 1, 10, 100, 1000)\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers for logistic regression\n",
    "    'max_iter': [1000, 10000, 20000],  # Number of iterations for convergence\n",
    "}\n",
    "\n",
    "# Define the custom scoring function (e.g., F1 score)\n",
    "custom_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Update n_iter to match the number of combinations\n",
    "random_search = RandomizedSearchCV(estimator=lr_country, \n",
    "                                   param_distributions=param_dist, \n",
    "                                   n_iter=100,  # Set n_iter to the number of parameter combinations\n",
    "                                   cv=5, \n",
    "                                   n_jobs=-1, \n",
    "                                   verbose=2, \n",
    "                                   scoring=custom_scorer, \n",
    "                                   random_state=42)\n",
    "\n",
    "# Fit the model with the scaled training data\n",
    "random_search.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "# Output the best parameters and the best F1 score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best F1 Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "61891a19-00ed-4608-aa9d-792489aade36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Country Name predicted_country_growth\n",
      "38935          Uzbekistan                Very High\n",
      "12044       United States                Very High\n",
      "29712          Kazakhstan                Very High\n",
      "11613      United Kingdom                Very High\n",
      "29640               Italy                Very High\n",
      "11255  Russian Federation                Very High\n",
      "11280              Sweden                Very High\n",
      "29902         Netherlands                Very High\n",
      "11319            Slovenia                Very High\n",
      "29866              Mexico                Very High\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found during the grid search\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Create the Logistic Regression model with the best parameters\n",
    "lr_country = LogisticRegression(\n",
    "    C=best_params['C'],\n",
    "    solver=best_params['solver'],\n",
    "    max_iter=best_params['max_iter'],\n",
    "    multi_class='ovr'  # Specify multiclass handling (One-vs-Rest)\n",
    ")\n",
    "\n",
    "# Initialize a container to store predictions with correct dtype (categorical or object)\n",
    "predicted_growth_values = pd.Series(index=opera.index, dtype='object')\n",
    "\n",
    "# Group the data by 'Country Name'\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Loop through each country group to train and predict separately\n",
    "for sub_region, group in sub_region_groups:\n",
    "    # If the group has more than one sample and more than one class, split and train\n",
    "    if len(group) > 1 and len(group['country_change_from_previous_season'].unique()) > 1:\n",
    "        # Extract the features and target for this country group\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['country_change_from_previous_season']\n",
    "\n",
    "        # Convert the target to categorical labels (same as before)\n",
    "        y_group_classified = pd.cut(y_group, bins=bins, labels=labels)\n",
    "\n",
    "        # Split data into train and test sets (80% train, 20% test)\n",
    "        X_train, X_test, y_train_country, y_test_country = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # **Only train if the target variable has more than one class**\n",
    "        if len(y_train_country.unique()) > 1:\n",
    "            # Train the model on the training data\n",
    "            lr_country.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "            # Scale the test data using the same scaler\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Make predictions for the test set\n",
    "            predictions = lr_country.predict(X_test_scaled)\n",
    "\n",
    "            # Assign predictions to the correct indices in the original DataFrame\n",
    "            predicted_growth_values.loc[X_test.index] = predictions  # Correctly assign predictions to test set rows\n",
    "        else:\n",
    "            print(f\"Skipping country: {sub_region} due to insufficient class variation in the target.\")\n",
    "            continue\n",
    "    else:\n",
    "        # Skip the group if it has only one class or one sample\n",
    "        print(f\"Skipping country: {sub_region} due to insufficient class variation or sample size.\")\n",
    "        continue\n",
    "\n",
    "# You can now inspect or save the results\n",
    "opera['predicted_country_growth'] = predicted_growth_values\n",
    "\n",
    "# Sort and get the top countries based on predicted growth\n",
    "df_country_growth_sorted = opera[['Country Name', 'predicted_country_growth']].sort_values(by='predicted_country_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each country\n",
    "df_opera_country_growth_unique = df_country_growth_sorted.drop_duplicates(subset='Country Name', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_countries_predicted_lr2 = df_opera_country_growth_unique.head(10)\n",
    "print(top_countries_predicted_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "76d9bb4c-7a6c-41d4-91c0-51474c7a98ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average Logistic Regression Metrics Across Sub-Regions for Countries using RandomizedSearch:\n",
      "   accuracy: 0.7520\n",
      "   precision: 0.7485\n",
      "   recall: 0.7520\n"
     ]
    }
   ],
   "source": [
    "lr_metrics_country2 = evaluate_model_performance(lr_country, X_country, y_country, sub_region_groups, scaler, bins, labels)\n",
    "\n",
    "print(f\"\\n🔹 Average Logistic Regression Metrics Across Sub-Regions for Countries using RandomizedSearch:\")\n",
    "for metric, value in lr_metrics_country2.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8abfb4a7-49c9-4047-ad72-b60589040b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 42 is smaller than n_iter=100. Running 42 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'solver': 'liblinear', 'max_iter': 1000, 'C': 0.1}\n",
      "Best F1 Score: 0.3973721239033546\n"
     ]
    }
   ],
   "source": [
    "# Initialize LogisticRegression\n",
    "lr_city = LogisticRegression(max_iter=20000, multi_class='ovr')\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 7),  # Exponential range for C (e.g., 0.001, 0.01, 0.1, 1, 10, 100, 1000)\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers for logistic regression\n",
    "    'max_iter': [1000, 10000, 20000],  # Number of iterations for convergence\n",
    "}\n",
    "\n",
    "# Define the custom scoring function (e.g., F1 score)\n",
    "custom_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# RandomizedSearchCV with custom scoring\n",
    "random_search = RandomizedSearchCV(estimator=lr_city, \n",
    "                                   param_distributions=param_dist, \n",
    "                                   n_iter=100,  # Number of random parameter combinations to try\n",
    "                                   cv=5,  # Number of folds in cross-validation\n",
    "                                   n_jobs=-1,  # Use all CPU cores\n",
    "                                   verbose=2,  # Print progress messages\n",
    "                                   scoring=custom_scorer,  # Use F1 score for evaluation\n",
    "                                   random_state=42)  # For reproducibility\n",
    "\n",
    "# Fit the model with the scaled training data\n",
    "random_search.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "# Output the best parameters and the best F1 score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best F1 Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2ce46d2-5b29-4784-9711-749a9824b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      iso             city predicted_city_growth\n",
      "38935  uz         Tashkent             Very High\n",
      "13716  au           Sydney             Very High\n",
      "13410  us     White Plains             Very High\n",
      "13422  us       Wilmington             Very High\n",
      "33126  au        Melbourne             Very High\n",
      "33004  at         Salzburg             Very High\n",
      "32940  at             Graz             Very High\n",
      "32984  at             Linz             Very High\n",
      "32959  at        Innsbruck             Very High\n",
      "13407  us  West Palm Beach             Very High\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found during the grid search\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Initialize LogisticRegression with the best parameters\n",
    "lr_city = LogisticRegression(\n",
    "    C=best_params['C'],\n",
    "    solver=best_params['solver'],\n",
    "    max_iter=best_params['max_iter'],\n",
    "    multi_class='ovr'  # Specify multiclass handling (One-vs-Rest)\n",
    ")\n",
    "\n",
    "# Initialize a container to store predictions with correct dtype (categorical or object)\n",
    "predicted_growth_values = pd.Series(index=opera.index, dtype='object')\n",
    "\n",
    "# Still group the data by 'sub-region'\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Loop through each sub-region group to train and predict separately for cities\n",
    "for sub_region, group in sub_region_groups:\n",
    "    # If the group has more than one sample and more than one class, split and train\n",
    "    if len(group) > 1 and len(group['city_change_from_previous_season'].unique()) > 1:\n",
    "        # Extract the features and target for this sub-region group\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['city_change_from_previous_season']\n",
    "\n",
    "        # Convert the target to categorical labels (same as before)\n",
    "        y_group_classified = pd.cut(y_group, bins=bins, labels=labels)\n",
    "\n",
    "        # Split data into train and test sets (80% train, 20% test)\n",
    "        X_train, X_test, y_train_city, y_test_city = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # **Only train if the target variable has more than one class**\n",
    "        if len(y_train_city.unique()) > 1:\n",
    "            # Train the model on the training data\n",
    "            lr_city.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "            # Scale the test data using the same scaler\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Make predictions for the test set\n",
    "            predictions = lr_city.predict(X_test_scaled)\n",
    "\n",
    "            # Ensure the indices of predictions and X_test match\n",
    "            predicted_growth_values.loc[X_test.index] = predictions  # Correctly assign predictions to test set rows\n",
    "        else:\n",
    "            print(f\"Skipping sub-region: {city_name} due to insufficient class variation in the target.\")\n",
    "            continue\n",
    "    else:\n",
    "        # Skip the group if it has only one class or one sample\n",
    "        print(f\"Skipping sub-region: {city_name} due to insufficient class variation or sample size.\")\n",
    "        continue\n",
    "\n",
    "# You can now inspect or save the results\n",
    "opera['predicted_city_growth'] = predicted_growth_values\n",
    "\n",
    "# Sort and get the top cities based on predicted growth\n",
    "df_city_growth_sorted = opera[['iso', 'city', 'predicted_city_growth']].sort_values(by='predicted_city_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each city\n",
    "df_opera_city_growth_unique = df_city_growth_sorted.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "# Get top 10 cities with highest predicted growth\n",
    "top_cities_predicted_lr2 = df_opera_city_growth_unique.head(10)\n",
    "print(top_cities_predicted_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eebab840-36b3-4e61-b692-8484c76d7865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average Logistic Regression Metrics Across Sub-Regions for Cities using RandomizedSearch:\n",
      "   accuracy: 0.4754\n",
      "   precision: 0.5147\n",
      "   recall: 0.4754\n"
     ]
    }
   ],
   "source": [
    "lr_metrics_city2 = evaluate_model_performance(lr_city, X_city, y_city, sub_region_groups, scaler, bins, labels)\n",
    "\n",
    "print(f\"\\n🔹 Average Logistic Regression Metrics Across Sub-Regions for Cities using RandomizedSearch:\")\n",
    "for metric, value in lr_metrics_city2.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8ea47-0a5f-4bf6-9fcd-60fbe54f5629",
   "metadata": {},
   "source": [
    "Random Forest Classifier since the models for countries and cities were not very strong with logistic regression and might work better with RCF since it's a stronger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1ada48c3-f655-443e-b52f-db2674fbcf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20, 'bootstrap': False}\n",
      "Best F1 Score: 0.997500699650175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Hyperparameter distributions for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(100, 1001, 100),  # Randomly sample between 100 and 500 estimators\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],  # Can sample between 'sqrt' and 'log2'\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_country = RandomForestClassifier()\n",
    "\n",
    "# RandomizedSearchCV with custom scoring (no n_iter)\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_country, \n",
    "                                   param_distributions=param_dist,  # Param distributions for RandomizedSearchCV\n",
    "                                   n_iter=100,  # Number of random combinations to sample\n",
    "                                   cv=5, \n",
    "                                   n_jobs=-1, \n",
    "                                   verbose=2, \n",
    "                                   random_state=42)\n",
    "\n",
    "# Fit the randomized search\n",
    "random_search_rf.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "# Output the best parameters and the best F1 score\n",
    "print(\"Best Parameters:\", random_search_rf.best_params_)\n",
    "print(\"Best F1 Score:\", random_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e4652333-06eb-43ba-bb6c-d13dc297332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Country Name predicted_country_growth\n",
      "8303              Germany                        3\n",
      "9958                Italy                        3\n",
      "9984                Japan                        3\n",
      "10060  Korea, Republic of                        3\n",
      "25642       United States                        3\n",
      "9374               France                        3\n",
      "10407              Poland                        3\n",
      "10166              Latvia                        3\n",
      "10250              Mexico                        3\n",
      "9536              Hungary                        3\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found during the grid search\n",
    "best_params = random_search_rf.best_params_\n",
    "\n",
    "# Initialize RandomForestClassifier with the best parameters\n",
    "rf_country = RandomForestClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    max_features=best_params['max_features'],\n",
    "    bootstrap=best_params['bootstrap'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize a container to store predictions with correct dtype (categorical or object)\n",
    "predicted_growth_values = pd.Series(index=opera.index, dtype='object')\n",
    "\n",
    "# Still group the data by 'sub-region'\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Initialize StandardScaler (to ensure consistent scaling)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through each country group to train and predict separately for cities\n",
    "for sub_region, group in sub_region_groups:\n",
    "    # If the group has more than one sample and more than one class, split and train\n",
    "    if len(group) > 1 and len(group['country_change_from_previous_season'].unique()) > 1:\n",
    "        # Extract the features and target for this country group\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['country_change_from_previous_season']\n",
    "\n",
    "        # Convert the target to categorical labels (same as before)\n",
    "        y_group_classified = pd.cut(y_group, bins=bins, labels=labels)\n",
    "\n",
    "        # Split data into train and test sets (80% train, 20% test)\n",
    "        X_train, X_test, y_train_country, y_test_country = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # **Only train if the target variable has more than one class**\n",
    "        if len(y_train_country.unique()) > 1:\n",
    "            # Train the Random Forest Classifier with best_params\n",
    "            rf_country.fit(X_train_scaled, y_train_country)  # ✅ Now fitting the model\n",
    "\n",
    "            # Scale the test data using the same scaler\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Make predictions for the test set\n",
    "            predictions = rf_country.predict(X_test_scaled)  # ✅ Using the fitted model\n",
    "\n",
    "            # Assign predictions to the correct indices in the original DataFrame\n",
    "            predicted_growth_values.loc[X_test.index] = predictions  # Correctly assign predictions to test set rows\n",
    "        else:\n",
    "            print(f\"Skipping the sub-region: {sub_region} due to insufficient class variation in the target.\")\n",
    "            continue\n",
    "    else:\n",
    "        # Skip the group if it has only one class or one sample\n",
    "        print(f\"Skipping the sub-region: {sub_region} due to insufficient class variation or sample size.\")\n",
    "        continue\n",
    "\n",
    "# You can now inspect or save the results\n",
    "opera['predicted_country_growth'] = predicted_growth_values\n",
    "df_country_growth_sorted = opera[['Country Name', 'predicted_country_growth']].sort_values(by='predicted_country_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each country\n",
    "df_opera_country_growth_unique = df_country_growth_sorted.drop_duplicates(subset='Country Name', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_countries_predicted_rf = df_opera_country_growth_unique.head(30)\n",
    "print(top_countries_predicted_rf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ad12b6d1-a61d-46ed-ab8d-00e60ba8e6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average Random Forest Metrics Across Sub-Regions for Countries\n",
      "   accuracy: 0.9632\n",
      "   precision: 0.9754\n",
      "   recall: 0.9632\n"
     ]
    }
   ],
   "source": [
    "rf_metrics_country = evaluate_model_performance(rf_country, X_country, y_country, sub_region_groups, scaler, bins, labels)\n",
    "\n",
    "print(f\"\\n🔹 Average Random Forest Metrics Across Sub-Regions for Countries\")\n",
    "for metric, value in rf_metrics_country.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "12a59313-7ce9-4874-8bb6-d74f21ed0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters: {'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None, 'bootstrap': False}\n",
      "Best F1 Score: 0.9095271364317842\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter distributions for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(100, 1001, 100),  # Randomly sample between 100 and 500 estimators\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],  # Can sample between 'sqrt' and 'log2'\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_city = RandomForestClassifier()\n",
    "\n",
    "# RandomizedSearchCV with custom scoring (no n_iter)\n",
    "random_search_rfy = RandomizedSearchCV(estimator=rf_city, \n",
    "                                   param_distributions=param_dist,  # Param distributions for RandomizedSearchCV\n",
    "                                   n_iter=100,  # Number of random combinations to sample\n",
    "                                   cv=5, \n",
    "                                   n_jobs=-1, \n",
    "                                   verbose=2, \n",
    "                                   random_state=42)\n",
    "\n",
    "# Fit the randomized search\n",
    "random_search_rfy.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "# Output the best parameters and the best F1 score\n",
    "print(\"Best Parameters:\", random_search_rfy.best_params_)\n",
    "print(\"Best F1 Score:\", random_search_rfy.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "db2a2f00-43e5-4845-83ed-fd3c06ea6f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      iso         city predicted_city_growth\n",
      "12268  us       Dayton                     3\n",
      "13311  us   Washington                     3\n",
      "30351  ru  Chelyabinsk                     3\n",
      "10389  nz   Berhampore                     3\n",
      "26930  ch        Basel                     3\n",
      "26913  ca    Vancouver                     3\n",
      "26825  bg        Varna                     3\n",
      "33564  cz      Liberec                     3\n",
      "30408  ru        Kazan                     3\n",
      "13447  uz     Tashkent                     3\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found during the grid search\n",
    "best_params = random_search_rfy.best_params_\n",
    "\n",
    "# Initialize RandomForestClassifier with the best parameters\n",
    "rf_city = RandomForestClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    max_features=best_params['max_features'],\n",
    "    bootstrap=best_params['bootstrap'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize a container to store predictions with correct dtype (categorical or object)\n",
    "predicted_growth_values = pd.Series(index=opera.index, dtype='object')\n",
    "\n",
    "# Still group the data by 'sub-region'\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Initialize StandardScaler (to ensure consistent scaling)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through each country group to train and predict separately for cities\n",
    "for sub_region, group in sub_region_groups:\n",
    "    # If the group has more than one sample and more than one class, split and train\n",
    "    if len(group) > 1 and len(group['city_change_from_previous_season'].unique()) > 1:\n",
    "        # Extract the features and target for this country group\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['city_change_from_previous_season']\n",
    "\n",
    "        # Convert the target to categorical labels (same as before)\n",
    "        y_group_classified = pd.cut(y_group, bins=bins, labels=labels)\n",
    "\n",
    "        # Split data into train and test sets (80% train, 20% test)\n",
    "        X_train, X_test, y_train_city, y_test_city = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # **Only train if the target variable has more than one class**\n",
    "        if len(y_train_city.unique()) > 1:\n",
    "            # Train the Random Forest Classifier with best_params\n",
    "            rf_city.fit(X_train_scaled, y_train_city)  # ✅ Now fitting the model\n",
    "\n",
    "            # Scale the test data using the same scaler\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Make predictions for the test set\n",
    "            predictions = rf_city.predict(X_test_scaled)  # ✅ Using the fitted model\n",
    "\n",
    "            # Assign predictions to the correct indices in the original DataFrame\n",
    "            predicted_growth_values.loc[X_test.index] = predictions  # Correctly assign predictions to test set rows\n",
    "        else:\n",
    "            print(f\"Skipping the sub-region: {sub_region} due to insufficient class variation in the target.\")\n",
    "            continue\n",
    "    else:\n",
    "        # Skip the group if it has only one class or one sample\n",
    "        print(f\"Skipping the sub-region: {sub_region} due to insufficient class variation or sample size.\")\n",
    "        continue\n",
    "\n",
    "# You can now inspect or save the results\n",
    "opera['predicted_city_growth'] = predicted_growth_values\n",
    "df_city_growth_sorted = opera[['iso', 'city', 'predicted_city_growth']].sort_values(by='predicted_city_growth', ascending=False)\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each country\n",
    "df_opera_city_growth_unique = df_city_growth_sorted.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_cities_predicted_rf = df_opera_city_growth_unique.head(30)\n",
    "print(top_cities_predicted_rf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "952f3b85-dffd-4802-8c0f-8a2cc621778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average Random Forest Metrics Across Sub-Regions for Cities:\n",
      "   accuracy: 0.8821\n",
      "   precision: 0.8975\n",
      "   recall: 0.8821\n"
     ]
    }
   ],
   "source": [
    "rf_metrics_city = evaluate_model_performance(rf_city, X_city, y_city, sub_region_groups, scaler, bins, labels)\n",
    "\n",
    "print(f\"\\n🔹 Average Random Forest Metrics Across Sub-Regions for Cities:\")\n",
    "for metric, value in rf_metrics_city.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33e1b8-1e05-4971-bbf1-df3e88ada42d",
   "metadata": {},
   "source": [
    "Random Forest was much stronger at predicting with accuracy and precision. I'm going to try one more model, the XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dfd3abb2-00f1-4725-9f08-03e170b33db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c9153c75-9576-4ab3-9212-92698fe28ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5ac77a85-6e1b-4ab5-acfb-a5e810df5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance_1(model, X, y, sub_region_groups, scaler, bins, labels):\n",
    "    \"\"\"\n",
    "    Trains and evaluates an XGBoost model for each sub-region, then returns the average accuracy, precision, and recall.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The XGBoost model to train (e.g., XGBClassifier).\n",
    "    - X: Feature DataFrame.\n",
    "    - y: Target variable DataFrame.\n",
    "    - sub_region_groups: Grouped data by sub-region.\n",
    "    - scaler: StandardScaler instance.\n",
    "    - bins: Bins for categorizing target values.\n",
    "    - labels: Labels corresponding to bins.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with average accuracy, precision, and recall.\n",
    "    \"\"\"\n",
    "    accuracies, precisions, recalls = [], [], []\n",
    "\n",
    "    for sub_region, group in sub_region_groups:\n",
    "        if len(group) > 1 and len(group['country_change_from_previous_season'].unique()) > 1:\n",
    "            # Extract features and target\n",
    "            X_group = group[X.columns]\n",
    "            y_group = group[y.name]\n",
    "\n",
    "            # Convert target variable to categorical labels (Ensure labels start from 0)\n",
    "            y_group_classified = pd.cut(y_group, bins=bins, labels=range(len(labels)))\n",
    "            y_group_classified = y_group_classified.astype(int) - y_group_classified.min()\n",
    "\n",
    "            # Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Scale the training and test data\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Handle missing classes (Ensure XGBoost knows all possible labels)\n",
    "            unique_classes = set(y_train.unique())\n",
    "            all_classes = set(range(len(labels)))\n",
    "            missing_classes = all_classes - unique_classes\n",
    "\n",
    "            if missing_classes:\n",
    "                for missing_class in missing_classes:\n",
    "                    dummy_X = X_train_scaled.mean(axis=0) + np.random.normal(0, 0.01, X_train_scaled.shape[1])\n",
    "                    dummy_X = dummy_X.reshape(1, -1)\n",
    "                    dummy_y = np.array([missing_class])\n",
    "\n",
    "                    X_train_scaled = np.vstack([X_train_scaled, dummy_X])\n",
    "                    y_train = np.append(y_train, dummy_y)\n",
    "\n",
    "            # Train only if there are multiple classes\n",
    "            if len(np.unique(y_train)) > 1:\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                predictions = model.predict(X_test_scaled)\n",
    "\n",
    "                # Store evaluation metrics\n",
    "                accuracies.append(accuracy_score(y_test, predictions))\n",
    "                precisions.append(precision_score(y_test, predictions, average='weighted', zero_division=0))\n",
    "                recalls.append(recall_score(y_test, predictions, average='weighted', zero_division=0))\n",
    "\n",
    "    # Compute the average metrics\n",
    "    avg_metrics_1 = {\n",
    "        'accuracy': np.mean(accuracies) if accuracies else 0,\n",
    "        'precision': np.mean(precisions) if precisions else 0,\n",
    "        'recall': np.mean(recalls) if recalls else 0\n",
    "    }\n",
    "\n",
    "    return avg_metrics_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3b3fb451-022e-487e-9182-7c2da19c01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables (predictors)\n",
    "X_country = opera[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
    "                'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', \n",
    "                'Year', 'Month', 'Day', 'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "\n",
    "# Dependent variable (growth at city level)\n",
    "y_country = opera['country_change_from_previous_season']\n",
    "\n",
    "# Compute quartiles for binning\n",
    "q1 = y_country.quantile(0.25)\n",
    "q2 = y_country.quantile(0.50)\n",
    "q3 = y_country.quantile(0.75)\n",
    "\n",
    "# Define bins and numeric labels\n",
    "bins = [-float('inf'), q1, q2, q3, float('inf')]\n",
    "labels = [0, 1, 2, 3]  # Low = 0, Medium = 1, High = 2, Very High = 3\n",
    "\n",
    "# Convert to categorical numeric labels\n",
    "y_country_classified = pd.cut(y_country, bins=bins, labels=labels).astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_country, y_test_country = train_test_split(X_country, y_country_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data (using the same scaler as the training data)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a1498f7-a056-42bc-8efd-e6f6ebee5f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.3s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.6s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.6s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   4.3s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   3.4s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=  16.4s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.4s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.5s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.4s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=   8.0s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  41.4s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  40.9s\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time= 3.8min\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.4s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   1.2s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   0.9s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   1.2s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=   9.9s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=  14.1s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  49.0s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=  10.5s\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time= 1.2min\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   7.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=   8.5s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  13.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   6.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.0s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   7.6s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   9.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   3.6s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   4.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   8.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   5.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   6.7s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   6.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  10.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   9.7s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   3.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=   8.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=   8.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   5.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   7.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   9.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  13.0s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   7.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   7.5s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  14.2s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  13.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   5.5s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time=   8.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   5.3s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=  10.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  11.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   4.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   3.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   6.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   7.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   7.4s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   6.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   4.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   4.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=  11.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  12.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  10.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  11.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=  16.7s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   7.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  26.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=  17.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   4.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=  16.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  11.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  10.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   7.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  28.5s\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.4s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.7s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.6s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   4.5s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   3.4s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=  16.3s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=  15.6s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.5s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  41.5s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   7.6s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.6s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.7s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.5s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.6s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.5s\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time= 3.8min\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   1.5s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=  10.0s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=  14.3s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  52.4s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 1.3min\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  10.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   7.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   5.9s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   4.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  12.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   7.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   6.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   7.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   3.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  13.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=   6.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   6.5s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  10.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   9.9s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   3.6s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  12.9s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=700; total time=   9.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   7.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   8.7s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=900; total time=   9.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=900; total time=   9.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   5.9s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  14.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=900; total time=   9.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time=   8.9s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   5.5s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=  10.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  11.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   3.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   3.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   7.0s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time=   5.0s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time=   5.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600; total time=   5.9s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=900; total time=  11.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   4.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=  10.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  17.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  11.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  12.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   9.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   9.4s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  27.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=  16.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  15.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   4.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=  16.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  19.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   7.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  29.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=  12.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  11.7s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   9.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=  13.9s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=  20.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  18.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  18.0s\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.3s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.6s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.6s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   4.4s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   3.4s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=  16.3s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=  15.7s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  41.7s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.6s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.6s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.5s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.6s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.5s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 3.8min\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time=  39.3s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.4s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   1.4s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   1.6s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=  10.7s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=   5.0s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=  10.4s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  51.8s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 1.3min\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  10.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  10.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=   8.7s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  12.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   7.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.0s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   7.7s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   7.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   8.9s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   9.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   6.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=   6.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=   7.5s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=   9.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  10.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   9.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   5.6s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  13.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=700; total time=   9.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   7.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   9.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=900; total time=   9.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   7.6s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   5.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  14.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=900; total time=   9.5s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   4.5s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=700; total time=   6.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  11.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=900; total time=   9.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   6.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=   9.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   7.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   8.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   5.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=900; total time=  11.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=  10.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  12.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  10.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  11.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=  16.9s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   7.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  29.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=  16.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  14.0s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=  16.3s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  18.9s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   7.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  28.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  10.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=  11.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   9.6s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  19.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=  11.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  19.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   7.4s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  23.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.6s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   4.3s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   3.6s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.9s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=  16.3s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.6s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.6s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.5s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.4s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  41.3s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  40.8s\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time= 3.8min\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.3s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   0.7s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   1.4s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.2s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.6s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  52.4s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   9.9s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 1.1min\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  10.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  10.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=   8.5s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  11.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   5.2s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   7.7s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   9.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   7.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  13.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=   6.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   5.0s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   6.1s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=  10.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.8s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.8s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   5.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  12.9s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=700; total time=   9.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   8.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   4.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  13.5s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   6.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   5.5s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  14.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=900; total time=   9.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time=   8.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   5.5s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=  10.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=700; total time=  10.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   4.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   3.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   3.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   6.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=   8.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   6.7s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   4.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   4.6s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   3.7s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  12.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  18.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  12.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   9.1s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   7.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  27.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  14.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   4.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  11.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  12.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  10.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   7.1s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   7.1s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=  18.9s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=  19.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  11.5s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  18.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=  11.3s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=  20.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   7.3s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.3s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.6s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.6s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   4.4s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   3.3s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.8s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=  16.0s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=   7.8s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  41.2s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   7.6s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 3.8min\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   1.1s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   1.8s\n",
      "[CV] END ...........C=10.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=  10.5s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=  11.5s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=   7.7s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  50.3s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .........C=1000.0, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time= 1.1min\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  10.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  10.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=   8.5s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   6.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  11.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   6.5s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   9.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   4.0s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   4.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  12.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=   6.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.3s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=  10.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  10.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   5.6s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   4.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=   8.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   5.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   7.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   8.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  12.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   7.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   4.2s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  14.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   8.1s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   2.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   5.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   5.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   3.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   5.4s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=  10.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  11.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=900; total time=  10.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   6.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=   8.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   7.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   8.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   5.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=900; total time=  11.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   4.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=  10.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  17.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  19.0s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  10.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   9.4s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  26.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  29.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   4.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  11.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  12.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  10.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   7.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   7.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=  19.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  10.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=  11.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  11.6s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  18.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=  14.5s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  19.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   7.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  23.4s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   8.6s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.6s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.6s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.3s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   4.4s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.3s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.9s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=  16.0s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=   8.1s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  41.4s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.6s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.7s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.8s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   7.7s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 3.8min\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.4s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   0.6s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   1.3s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   3.1s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.2s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=100.0, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=   8.8s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  50.5s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   7.4s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time=  28.0s\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time=  55.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   7.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   7.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=   8.6s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  11.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   7.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   9.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   4.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  13.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   6.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   6.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=  10.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   5.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  13.1s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=700; total time=   9.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   7.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  13.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   7.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   5.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  14.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   2.7s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   2.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   2.7s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=900; total time=   9.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time=   8.7s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   4.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=700; total time=   6.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=900; total time=  10.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=   9.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   7.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   8.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600; total time=   5.7s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   6.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=900; total time=  11.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=  10.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  17.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  18.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=  17.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.5s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   7.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  25.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=  17.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   4.5s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=  16.4s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  19.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   7.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  28.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  10.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.7s\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .................C=0.01, max_iter=1000, solver=saga; total time=   0.3s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.01, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.6s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.6s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   4.4s\n",
      "[CV] END ............C=1.0, max_iter=20000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=20000, solver=saga; total time=   3.5s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.8s\n",
      "[CV] END ...........C=10.0, max_iter=20000, solver=liblinear; total time=   0.3s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=  16.3s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=   7.8s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.5s\n",
      "[CV] END ..........C=100.0, max_iter=20000, solver=liblinear; total time=   0.4s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  41.4s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   7.6s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 3.8min\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.4s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.3s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END .................C=0.1, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   1.2s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   1.8s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=   4.2s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=   9.8s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=100.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  50.5s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   7.4s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   2.7s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .........C=1000.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 1.1min\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  10.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  10.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   5.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   6.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  12.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  11.7s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   7.6s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   9.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   3.6s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   4.9s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   8.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   9.5s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   6.7s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=   7.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   9.9s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   9.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   9.7s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=   8.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   5.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   7.6s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  12.8s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   7.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   5.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  14.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   7.9s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   2.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   5.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   5.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   3.7s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=700; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=700; total time=   9.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=900; total time=  10.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   6.7s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time=   5.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   7.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   5.7s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   6.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=900; total time=  11.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   4.5s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   3.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  17.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  18.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=  17.1s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  26.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  28.7s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  12.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=  11.4s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  19.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  10.3s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   7.1s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=  19.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=  19.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.7s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  11.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   9.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=  13.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=  11.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  19.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  23.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  18.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   7.8s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  24.2s\n",
      "[CV] END ...........C=0.001, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.2s\n",
      "[CV] END ..........C=0.001, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ............C=0.01, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.01, max_iter=10000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.3s\n",
      "[CV] END ................C=0.01, max_iter=20000, solver=saga; total time=   0.3s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.7s\n",
      "[CV] END .................C=0.1, max_iter=10000, solver=saga; total time=   0.6s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=1.0, max_iter=1000, solver=saga; total time=   4.5s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   3.5s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.9s\n",
      "[CV] END ................C=10.0, max_iter=10000, solver=saga; total time=  16.1s\n",
      "[CV] END ................C=100.0, max_iter=1000, solver=saga; total time=   8.0s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  41.1s\n",
      "[CV] END ..........C=1000.0, max_iter=1000, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=1000.0, max_iter=1000, solver=saga; total time=   7.7s\n",
      "[CV] END ..............C=1000.0, max_iter=10000, solver=saga; total time= 3.8min\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time=  36.9s\n",
      "[CV] END ................C=0.001, max_iter=1000, solver=saga; total time=   0.4s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.001, max_iter=20000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END ................C=0.01, max_iter=10000, solver=saga; total time=   0.2s\n",
      "[CV] END .............C=0.1, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=0.1, max_iter=1000, solver=saga; total time=   0.3s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.1, max_iter=20000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END .............C=1.0, max_iter=1000, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=1.0, max_iter=10000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=1.0, max_iter=10000, solver=saga; total time=   1.3s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END ............C=10.0, max_iter=1000, solver=liblinear; total time=   0.2s\n",
      "[CV] END .................C=10.0, max_iter=1000, solver=saga; total time=   7.7s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=   4.5s\n",
      "[CV] END ................C=10.0, max_iter=20000, solver=saga; total time=  11.0s\n",
      "[CV] END ...............C=100.0, max_iter=10000, solver=saga; total time=  16.2s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  17.1s\n",
      "[CV] END ...............C=100.0, max_iter=20000, solver=saga; total time=  52.4s\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time=  26.8s\n",
      "[CV] END ..............C=1000.0, max_iter=20000, solver=saga; total time=  47.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  10.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.1s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   5.9s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  12.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  11.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   6.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=   7.1s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  12.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=   6.6s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   5.0s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=   6.1s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=  10.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   5.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=  12.4s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   5.6s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  12.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=700; total time=   9.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   8.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=900; total time=   9.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=900; total time=   9.2s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   7.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  14.0s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  13.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=900; total time=   9.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   4.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   5.4s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=  10.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  11.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=900; total time=   9.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   6.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=   9.0s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   7.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   6.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   4.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   4.6s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   3.7s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=  17.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=900; total time=  19.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=900; total time=  17.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   7.5s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  29.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=  15.9s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  14.3s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=  16.4s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=700; total time=  19.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.4s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   7.3s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  28.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  10.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=  11.2s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   9.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=  13.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=700; total time=  14.8s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=  20.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=  18.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=  18.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=   8.7s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=  24.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.4s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=700; total time=  16.7s\n",
      "[CV] END bootstrap=False, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=  15.1s\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=700; total time=  17.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.9s\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  18.0s\n",
      "Best Parameters: {'subsample': 0.875, 'reg_lambda': 1.0, 'reg_alpha': 0.01, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.1711111111111111, 'gamma': 0.25, 'colsample_bytree': 0.5}\n",
      "Best F1 Score: 0.9966299707264021\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 500, 50),  # Number of trees\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),  # Learning rate\n",
    "    'max_depth': np.arange(3, 10, 1),  # Tree depth\n",
    "    'min_child_weight': np.arange(1, 10, 2),  # Minimum sum of instance weight needed in a child\n",
    "    'gamma': np.linspace(0, 0.5, 5),  # Minimum loss reduction to make further partition\n",
    "    'subsample': np.linspace(0.5, 1.0, 5),  # Fraction of samples used per tree\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, 5),  # Fraction of features used per tree\n",
    "    'reg_alpha': np.logspace(-3, 1, 5),  # L1 regularization\n",
    "    'reg_lambda': np.logspace(-3, 1, 5),  # L2 regularization\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_country = xgb.XGBClassifier(objective='multi:softmax', num_class=3, random_state=42)\n",
    "\n",
    "# Define a custom scoring function (F1-score)\n",
    "custom_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Perform Randomized Search\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb_country,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of random samples\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring=custom_scorer,\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Randomized Search on training data (for country)\n",
    "random_search_xgb.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "# Output the best parameters and best F1 score\n",
    "print(\"Best Parameters:\", random_search_xgb.best_params_)\n",
    "print(\"Best F1 Score:\", random_search_xgb.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "68c0d157-08f6-49cf-b41b-180fd76456f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Country Name predicted_country_growth\n",
      "8306              Germany                        3\n",
      "9965                Italy                        3\n",
      "9984                Japan                        3\n",
      "10060  Korea, Republic of                        3\n",
      "25649       United States                        3\n",
      "10166              Latvia                        3\n",
      "10468              Poland                        3\n",
      "10250              Mexico                        3\n",
      "9536              Hungary                        3\n",
      "9394               France                        3\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found during the grid search\n",
    "best_params = random_search_xgb.best_params_\n",
    "\n",
    "# Initialize XGBoost with the best parameters\n",
    "xgb_country = xgb.XGBClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    reg_alpha=best_params['reg_alpha'],\n",
    "    reg_lambda=best_params['reg_lambda'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize a container to store predictions with correct dtype (categorical or object)\n",
    "predicted_growth_values = pd.Series(index=opera.index, dtype='object')\n",
    "\n",
    "# Still group the data by 'sub-region'\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Initialize StandardScaler (to ensure consistent scaling)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through each sub-region to train and predict separately\n",
    "for sub_region, group in sub_region_groups:\n",
    "    # If the group has more than one sample and more than one class, split and train\n",
    "    if len(group) > 1 and len(group['country_change_from_previous_season'].unique()) > 1:\n",
    "        # Extract features and target\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['country_change_from_previous_season']\n",
    "\n",
    "        # Convert the target to categorical labels (Ensuring labels start from 0)\n",
    "        y_group_classified = pd.cut(y_group, bins=bins, labels=range(len(labels)))\n",
    "        \n",
    "        # Convert to integer type (XGBoost requires integer labels)\n",
    "        y_group_classified = y_group_classified.astype(int)\n",
    "\n",
    "        # Split into train-test sets (80-20 split)\n",
    "        X_train, X_test, y_train_country, y_test_country = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)  # Use same scaler for test data\n",
    "\n",
    "        # Ensure all possible classes are included in training, even if missing\n",
    "        if len(y_train_country.unique()) > 1:\n",
    "            missing_classes = set(range(len(labels))) - set(y_train_country.unique())\n",
    "\n",
    "            if missing_classes:\n",
    "                for missing_class in missing_classes:\n",
    "                    dummy_X = X_train_scaled.mean(axis=0) + np.random.normal(0, 0.01, X_train_scaled.shape[1])  # Add noise\n",
    "                    dummy_X = dummy_X.reshape(1, -1)  \n",
    "                    dummy_y = np.array([missing_class])\n",
    "\n",
    "                    X_train_scaled = np.vstack([X_train_scaled, dummy_X])\n",
    "                    y_train_country = np.append(y_train_country, dummy_y)\n",
    "\n",
    "            # Train the model\n",
    "            xgb_country.fit(X_train_scaled, y_train_country)\n",
    "\n",
    "            # Make predictions\n",
    "            predictions = xgb_country.predict(X_test_scaled)\n",
    "\n",
    "            # Assign predictions to correct indices in the original DataFrame\n",
    "            predicted_growth_values.loc[X_test.index] = predictions\n",
    "        else:\n",
    "            print(f\"Skipping {sub_region} due to insufficient class variation.\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Skipping {sub_region} due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "# Fill NaN values in case of skipped regions\n",
    "opera['predicted_country_growth'] = predicted_growth_values.fillna(\"No Prediction\")\n",
    "\n",
    "# Convert predictions to numeric where possible, keeping \"No Prediction\" as-is\n",
    "opera['numeric_growth'] = pd.to_numeric(opera['predicted_country_growth'], errors='coerce')\n",
    "\n",
    "# Sort using numeric values (NaNs get pushed to the bottom)\n",
    "df_country_growth_sorted = opera[['Country Name', 'predicted_country_growth', 'numeric_growth']].sort_values(by='numeric_growth', ascending=False)\n",
    "\n",
    "# Drop temp numeric column after sorting\n",
    "df_country_growth_sorted = df_country_growth_sorted.drop(columns='numeric_growth')\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each country\n",
    "df_opera_country_growth_unique = df_country_growth_sorted.drop_duplicates(subset='Country Name', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_countries_predicted_xgb = df_opera_country_growth_unique.head(30)\n",
    "print(top_countries_predicted_xgb.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b6d0673c-bfc4-45f7-94f2-f0db84d1ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average XGBoost Metrics Across Sub-Regions for Countries\n",
      "   accuracy: 0.9910\n",
      "   precision: 0.9899\n",
      "   recall: 0.9910\n"
     ]
    }
   ],
   "source": [
    "xgb_metrics_country = evaluate_model_performance_1(xgb_country, X_country, y_country, sub_region_groups, scaler, bins, labels)\n",
    "\n",
    "print(f\"\\n🔹 Average XGBoost Metrics Across Sub-Regions for Countries\")\n",
    "for metric, value in xgb_metrics_country.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e8df27c7-c573-4789-aca2-2bd2b8454deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique y_city_classified: [2 1 3 0]\n",
      "NaN count in y_city_classified: 0\n",
      "X_train_scaled shape: (10003, 12)\n",
      "y_train_city shape: (10003,)\n",
      "Best params from RandomizedSearch: {'subsample': 0.875, 'reg_lambda': 0.01, 'reg_alpha': 0.1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 8, 'learning_rate': 0.3, 'gamma': 0.125, 'colsample_bytree': 0.625}\n",
      "Predicted growth values (sample): [2 1 3 0]\n",
      "Sorted city growth data (first 10 rows):\n",
      "Empty DataFrame\n",
      "Columns: [iso, city, predicted_city_growth]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique y_city_classified:\", y_city_classified.unique())\n",
    "print(\"NaN count in y_city_classified:\", y_city_classified.isna().sum())\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"y_train_city shape:\", y_train_city.shape)\n",
    "print(\"Best params from RandomizedSearch:\", random_search_xgby.best_params_)\n",
    "print(\"Predicted growth values (sample):\", predicted_growth_values.dropna().unique())\n",
    "print(\"Sorted city growth data (first 10 rows):\")\n",
    "print(df_city_growth_sorted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "00224151-d53c-42f3-894b-bb906228cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables (predictors)\n",
    "X_city = opera[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop',\n",
    "                'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', \n",
    "                'Year', 'Month', 'Day', 'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "\n",
    "# Dependent variable (growth at city level)\n",
    "y_city = opera['city_change_from_previous_season']\n",
    "\n",
    "# Compute quartiles for binning\n",
    "q1 = y_city.quantile(0.25)\n",
    "q2 = y_city.quantile(0.50)\n",
    "q3 = y_city.quantile(0.75)\n",
    "\n",
    "# Define bins and numeric labels\n",
    "bins = [-float('inf'), q1, q2, q3, float('inf')]\n",
    "labels = [0, 1, 2, 3]  # Low = 0, Medium = 1, High = 2, Very High = 3\n",
    "\n",
    "# Convert to categorical numeric labels\n",
    "y_city_classified = pd.cut(y_city, bins=bins, labels=labels).astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_city, y_test_city = train_test_split(X_city, y_city_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data (using the same scaler as the training data)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3cbd1add-ed04-4c4f-b836-e94a88c65022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters: {'subsample': 0.875, 'reg_lambda': 0.01, 'reg_alpha': 0.1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 8, 'learning_rate': 0.3, 'gamma': 0.125, 'colsample_bytree': 0.625}\n",
      "Best F1 Score: 0.9225022006766685\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 500, 50),  # Number of trees\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),  # Learning rate\n",
    "    'max_depth': np.arange(3, 10, 1),  # Tree depth\n",
    "    'min_child_weight': np.arange(1, 10, 2),  # Minimum sum of instance weight needed in a child\n",
    "    'gamma': np.linspace(0, 0.5, 5),  # Minimum loss reduction to make further partition\n",
    "    'subsample': np.linspace(0.5, 1.0, 5),  # Fraction of samples used per tree\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, 5),  # Fraction of features used per tree\n",
    "    'reg_alpha': np.logspace(-3, 1, 5),  # L1 regularization\n",
    "    'reg_lambda': np.logspace(-3, 1, 5),  # L2 regularization\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_city = xgb.XGBClassifier(objective='multi:softmax', num_class=3, random_state=42)\n",
    "\n",
    "# Define a custom scoring function (F1-score)\n",
    "custom_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Perform Randomized Search\n",
    "random_search_xgby = RandomizedSearchCV(\n",
    "    estimator=xgb_city,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of random samples\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring=custom_scorer,\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Randomized Search on training data (for country)\n",
    "random_search_xgby.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "# Output the best parameters and best F1 score\n",
    "print(\"Best Parameters:\", random_search_xgby.best_params_)\n",
    "print(\"Best F1 Score:\", random_search_xgby.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7b5edf47-d677-494e-b069-7ac7f3ba16da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      iso         city predicted_city_growth\n",
      "33941  de      Dresden                     3\n",
      "13362  us   Washington                     3\n",
      "26988  ch       Luzern                     3\n",
      "33680  de     Augsburg                     3\n",
      "30456  ru       Moscow                     3\n",
      "30427  ru  Krasnoyarsk                     3\n",
      "30413  ru        Kazan                     3\n",
      "10462  pl         Lodz                     3\n",
      "16300  hu     Budapest                     3\n",
      "4536   sk   Bratislava                     3\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found during the grid search\n",
    "best_params = random_search_xgby.best_params_\n",
    "\n",
    "# Initialize XGBoost with the best parameters\n",
    "xgb_city = xgb.XGBClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    reg_alpha=best_params['reg_alpha'],\n",
    "    reg_lambda=best_params['reg_lambda'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize a container to store predictions with correct dtype (categorical or object)\n",
    "predicted_growth_values = pd.Series(index=opera.index, dtype='object')\n",
    "\n",
    "# Still group the data by 'sub-region'\n",
    "sub_region_groups = opera.groupby('sub-region')\n",
    "\n",
    "# Initialize StandardScaler (to ensure consistent scaling)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through each sub-region to train and predict separately\n",
    "for sub_region, group in sub_region_groups:\n",
    "    # If the group has more than one sample and more than one class, split and train\n",
    "    if len(group) > 1 and len(group['city_change_from_previous_season'].unique()) > 1:\n",
    "        # Extract features and target\n",
    "        X_group = group[['country population', 'performances_season_by_country', 'perf_per_10k_ppl_co_pop', \n",
    "                         'city population', 'performances_season_by_city', 'perf_per_1k_ppl_city_pop', 'Year', 'Month', 'Day', \n",
    "                         'Weekday', 'Week_of_Year', 'Days_Since_Start']]\n",
    "        y_group = group['city_change_from_previous_season']\n",
    "\n",
    "        # Convert the target to categorical labels (Ensuring labels start from 0)\n",
    "        y_group_classified = pd.cut(y_group, bins=bins, labels=range(len(labels)))\n",
    "        \n",
    "        # Convert to integer type (XGBoost requires integer labels)\n",
    "        y_group_classified = y_group_classified.astype(int)\n",
    "\n",
    "        # Split into train-test sets (80-20 split)\n",
    "        X_train, X_test, y_train_city, y_test_city = train_test_split(X_group, y_group_classified, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)  # Use same scaler for test data\n",
    "\n",
    "        # Ensure all possible classes are included in training, even if missing\n",
    "        if len(y_train_city.unique()) > 1:\n",
    "            missing_classes = set(range(len(labels))) - set(y_train_city.unique())\n",
    "\n",
    "            if missing_classes:\n",
    "                for missing_class in missing_classes:\n",
    "                    dummy_X = X_train_scaled.mean(axis=0) + np.random.normal(0, 0.01, X_train_scaled.shape[1])  # Add noise\n",
    "                    dummy_X = dummy_X.reshape(1, -1)  \n",
    "                    dummy_y = np.array([missing_class])\n",
    "\n",
    "                    X_train_scaled = np.vstack([X_train_scaled, dummy_X])\n",
    "                    y_train_city = np.append(y_train_city, dummy_y)\n",
    "\n",
    "            # Train the model\n",
    "            xgb_city.fit(X_train_scaled, y_train_city)\n",
    "\n",
    "            # Make predictions\n",
    "            predictions = xgb_city.predict(X_test_scaled)\n",
    "\n",
    "            # Assign predictions to correct indices in the original DataFrame\n",
    "            predicted_growth_values.loc[X_test.index] = predictions\n",
    "        else:\n",
    "            print(f\"Skipping {sub_region} due to insufficient class variation.\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Skipping {sub_region} due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "# Fill NaN values in case of skipped regions\n",
    "opera['predicted_city_growth'] = predicted_growth_values.fillna(\"No Prediction\")\n",
    "\n",
    "# Convert predictions to numeric where possible, keeping \"No Prediction\" as-is\n",
    "opera['numeric_growth'] = pd.to_numeric(opera['predicted_city_growth'], errors='coerce')\n",
    "\n",
    "# Sort using numeric values (NaNs get pushed to the bottom)\n",
    "df_city_growth_sorted = opera[['iso', 'city', 'predicted_city_growth', 'numeric_growth']].sort_values(by='numeric_growth', ascending=False)\n",
    "\n",
    "# Drop temp numeric column after sorting\n",
    "df_city_growth_sorted = df_city_growth_sorted.drop(columns='numeric_growth')\n",
    "\n",
    "# Drop duplicates and keep the row with the highest growth for each country\n",
    "df_opera_city_growth_unique = df_city_growth_sorted.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "# Get top 10 countries with highest predicted growth\n",
    "top_cities_predicted_xgb = df_opera_city_growth_unique.head(30)\n",
    "print(top_cities_predicted_xgb.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0ac01b86-64af-4ec7-96ab-2b393c15231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Average XGBoost Metrics Across Sub-Regions for Countries\n",
      "   accuracy: 0.8800\n",
      "   precision: 0.8956\n",
      "   recall: 0.8800\n"
     ]
    }
   ],
   "source": [
    "xgb_metrics_city = evaluate_model_performance_1(xgb_city, X_city, y_city, sub_region_groups, scaler, bins, labels)\n",
    "\n",
    "print(f\"\\n🔹 Average XGBoost Metrics Across Sub-Regions for Countries\")\n",
    "for metric, value in xgb_metrics_city.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e0d0ea8c-a9e2-486f-8efb-d63cb9533864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           MAE        RMSE        R2  accuracy  precision  \\\n",
      "lin_metrics_country  62.530724  103.077740 -1.504155       NaN        NaN   \n",
      "lr_metrics_country1        NaN         NaN       NaN  0.829187   0.824153   \n",
      "lr_metrics_country2        NaN         NaN       NaN  0.752030   0.748506   \n",
      "rf_metrics_country         NaN         NaN       NaN  0.963155   0.975437   \n",
      "xgb_metrics_country        NaN         NaN       NaN  0.991004   0.989886   \n",
      "lin_metrics_city     12.795362   19.067106 -4.087927       NaN        NaN   \n",
      "lr_metrics_city1           NaN         NaN       NaN  0.520271   0.541099   \n",
      "lr_metrics_city2           NaN         NaN       NaN  0.475384   0.514653   \n",
      "rf_metrics_city            NaN         NaN       NaN  0.882146   0.897475   \n",
      "xgb_metrics_city           NaN         NaN       NaN  0.879952   0.895552   \n",
      "\n",
      "                       recall  \n",
      "lin_metrics_country       NaN  \n",
      "lr_metrics_country1  0.829187  \n",
      "lr_metrics_country2  0.752030  \n",
      "rf_metrics_country   0.963155  \n",
      "xgb_metrics_country  0.991004  \n",
      "lin_metrics_city          NaN  \n",
      "lr_metrics_city1     0.520271  \n",
      "lr_metrics_city2     0.475384  \n",
      "rf_metrics_city      0.882146  \n",
      "xgb_metrics_city     0.879952  \n"
     ]
    }
   ],
   "source": [
    "# These variables are already defined in your environment\n",
    "metric_vars = [\n",
    "    \"lin_metrics_country\", \"lr_metrics_country1\", \"lr_metrics_country2\", \n",
    "    \"rf_metrics_country\", \"xgb_metrics_country\", \"lin_metrics_city\", \n",
    "    \"lr_metrics_city1\", \"lr_metrics_city2\", \"rf_metrics_city\", \"xgb_metrics_city\"\n",
    "]\n",
    "\n",
    "# Create a dictionary to store the metric values\n",
    "metrics_data = {}\n",
    "\n",
    "# Iterate through each metric variable and retrieve its values dynamically\n",
    "for var in metric_vars:\n",
    "    if var in globals():  # Check if the variable exists in the global scope\n",
    "        metrics_data[var] = globals()[var]  # Retrieve its value\n",
    "\n",
    "# Convert to DataFrame with NaN where values are missing\n",
    "final_df = pd.DataFrame(metrics_data).T\n",
    "\n",
    "# Reorder columns for clarity\n",
    "final_df = final_df[[\"MAE\", \"RMSE\", \"R2\", \"accuracy\", \"precision\", \"recall\"]]\n",
    "\n",
    "# Display the final structured DataFrame\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "cc66e809-0ff5-461e-9234-68b086051ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Countries Predictions (Grouped by Country):\n",
      "          Country Name  predicted_country_growth Model\n",
      "28           Argentina                         3  Both\n",
      "27           Australia                         3  Both\n",
      "22             Belgium                         3  Both\n",
      "26               China                         3  Both\n",
      "15      Czech Republic                         3  Both\n",
      "18             Denmark                         3  Both\n",
      "17             Estonia                         3  Both\n",
      "5               France                         3  Both\n",
      "0              Germany                         3  Both\n",
      "9              Hungary                         3  Both\n",
      "20             Ireland                         3  Both\n",
      "1                Italy                         3  Both\n",
      "2                Japan                         3  Both\n",
      "3   Korea, Republic of                         3  Both\n",
      "7               Latvia                         3  Both\n",
      "8               Mexico                         3  Both\n",
      "19         Netherlands                         3  Both\n",
      "23         New Zealand                         3  Both\n",
      "25              Norway                         3  Both\n",
      "6               Poland                         3  Both\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add 'Model' column\n",
    "top_countries_predicted_rf = top_countries_predicted_rf.copy()\n",
    "top_countries_predicted_rf['Model'] = 'RF - Country'\n",
    "\n",
    "top_countries_predicted_xgb = top_countries_predicted_xgb.copy()\n",
    "top_countries_predicted_xgb['Model'] = 'XGB - Country'\n",
    "\n",
    "# Step 2: Ensure no duplicates before merging and take more countries from each model\n",
    "top_countries_predicted_rf = top_countries_predicted_rf.head(50).drop_duplicates(subset=['Country Name'])\n",
    "top_countries_predicted_xgb = top_countries_predicted_xgb.head(50).drop_duplicates(subset=['Country Name'])\n",
    "\n",
    "# Step 3: Merge RF and XGB predictions based on 'Country Name'\n",
    "merged = pd.merge(\n",
    "    top_countries_predicted_rf, \n",
    "    top_countries_predicted_xgb, \n",
    "    on=['Country Name'], \n",
    "    how='outer', \n",
    "    suffixes=('_rf', '_xgb')\n",
    ")\n",
    "\n",
    "# Step 4: Convert growth values to numeric to avoid TypeErrors\n",
    "merged['predicted_country_growth_rf'] = pd.to_numeric(merged['predicted_country_growth_rf'], errors='coerce')\n",
    "merged['predicted_country_growth_xgb'] = pd.to_numeric(merged['predicted_country_growth_xgb'], errors='coerce')\n",
    "\n",
    "# Step 5: Determine the correct 'Model' label\n",
    "merged['Model'] = merged.apply(\n",
    "    lambda row: 'Both' if not pd.isna(row['Model_rf']) and not pd.isna(row['Model_xgb'])\n",
    "    else row['Model_rf'] if not pd.isna(row['Model_rf']) \n",
    "    else row['Model_xgb'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 6: Use the highest predicted_country_growth from RF or XGB\n",
    "merged['predicted_country_growth'] = merged[['predicted_country_growth_rf', 'predicted_country_growth_xgb']].max(axis=1)\n",
    "\n",
    "# Step 7: Select only necessary columns\n",
    "top_30_countries = merged[['Country Name', 'predicted_country_growth', 'Model']]\n",
    "\n",
    "# Step 8: Fill missing growth values (optional, depending on data)\n",
    "top_30_countries = top_30_countries.copy()\n",
    "top_30_countries.loc[:, 'predicted_country_growth'] = top_30_countries['predicted_country_growth'].fillna(0)\n",
    "\n",
    "# Step 9: Sort by predicted growth (descending) and then by country name\n",
    "top_30_countries = top_30_countries.sort_values(by=['predicted_country_growth', 'Country Name'], ascending=[False, True])\n",
    "\n",
    "# Step 10: Select top 20 unique countries\n",
    "top_20_countries = top_30_countries.head(20)\n",
    "\n",
    "# Display results\n",
    "print(\"Top 20 Countries Predictions (Grouped by Country):\")\n",
    "print(top_20_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6ae3ab15-2f39-4968-9294-a6db291846d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 Countries Predictions (Grouped by Country and Alphabetical Order ):\n",
      "          Country Name  predicted_country_growth Model\n",
      "28           Argentina                         3  Both\n",
      "27           Australia                         3  Both\n",
      "22             Belgium                         3  Both\n",
      "26               China                         3  Both\n",
      "15      Czech Republic                         3  Both\n",
      "18             Denmark                         3  Both\n",
      "17             Estonia                         3  Both\n",
      "5               France                         3  Both\n",
      "0              Germany                         3  Both\n",
      "9              Hungary                         3  Both\n",
      "20             Ireland                         3  Both\n",
      "1                Italy                         3  Both\n",
      "2                Japan                         3  Both\n",
      "3   Korea, Republic of                         3  Both\n",
      "7               Latvia                         3  Both\n",
      "8               Mexico                         3  Both\n",
      "19         Netherlands                         3  Both\n",
      "23         New Zealand                         3  Both\n",
      "25              Norway                         3  Both\n",
      "6               Poland                         3  Both\n",
      "10  Russian Federation                         3  Both\n",
      "14              Serbia                         3  Both\n",
      "21            Slovakia                         3  Both\n",
      "11            Slovenia                         3  Both\n",
      "16               Spain                         3  Both\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add 'Model' column\n",
    "top_countries_predicted_rf = top_countries_predicted_rf.copy()\n",
    "top_countries_predicted_rf['Model'] = 'RF - Country'\n",
    "\n",
    "top_countries_predicted_xgb = top_countries_predicted_xgb.copy()\n",
    "top_countries_predicted_xgb['Model'] = 'XGB - Country'\n",
    "\n",
    "# Step 2: Ensure no duplicates before merging and take more countries from each model\n",
    "top_countries_predicted_rf = top_countries_predicted_rf.head(50).drop_duplicates(subset=['Country Name'])\n",
    "top_countries_predicted_xgb = top_countries_predicted_xgb.head(50).drop_duplicates(subset=['Country Name'])\n",
    "\n",
    "# Step 3: Merge RF and XGB predictions based on 'Country Name'\n",
    "merged = pd.merge(\n",
    "    top_countries_predicted_rf, \n",
    "    top_countries_predicted_xgb, \n",
    "    on=['Country Name'], \n",
    "    how='outer', \n",
    "    suffixes=('_rf', '_xgb')\n",
    ")\n",
    "\n",
    "# Step 4: Convert growth values to numeric to avoid TypeErrors\n",
    "merged['predicted_country_growth_rf'] = pd.to_numeric(merged['predicted_country_growth_rf'], errors='coerce')\n",
    "merged['predicted_country_growth_xgb'] = pd.to_numeric(merged['predicted_country_growth_xgb'], errors='coerce')\n",
    "\n",
    "# Step 5: Determine the correct 'Model' label\n",
    "merged['Model'] = merged.apply(\n",
    "    lambda row: 'Both' if not pd.isna(row['Model_rf']) and not pd.isna(row['Model_xgb'])\n",
    "    else row['Model_rf'] if not pd.isna(row['Model_rf']) \n",
    "    else row['Model_xgb'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 6: Use the highest predicted_country_growth from RF or XGB\n",
    "merged['predicted_country_growth'] = merged[['predicted_country_growth_rf', 'predicted_country_growth_xgb']].max(axis=1)\n",
    "\n",
    "# Step 7: Select only necessary columns\n",
    "top_30_countries = merged[['Country Name', 'predicted_country_growth', 'Model']]\n",
    "\n",
    "# Step 8: Fill missing growth values (optional, depending on data)\n",
    "top_30_countries = top_30_countries.copy()\n",
    "top_30_countries.loc[:, 'predicted_country_growth'] = top_30_countries['predicted_country_growth'].fillna(0)\n",
    "\n",
    "# Step 9: Sort by predicted growth (descending) and then by country name\n",
    "top_30_countries = top_30_countries.sort_values(by=['predicted_country_growth', 'Country Name'], ascending=[False, True])\n",
    "\n",
    "# Step 10: Select top 20 unique countries\n",
    "top_25_countries = top_30_countries.head(25)\n",
    "\n",
    "# Display results\n",
    "print(\"Top 25 Countries Predictions (Grouped by Country and Alphabetical Order ):\")\n",
    "print(top_25_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8539860c-6c2a-4cf3-bd68-16b81d073638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 Cities Predictions Alphabetically:\n",
      "   iso               city  predicted_city_growth       Model\n",
      "35  de  Annaberg-Buchholz                    3.0  XGB - City\n",
      "25  de           Augsburg                    3.0        Both\n",
      "34  de       Bad Hersfeld                    3.0  XGB - City\n",
      "33  de        Baden-Baden                    3.0  XGB - City\n",
      "4   ch              Basel                    3.0        Both\n",
      "3   nz         Berhampore                    3.0   RF - City\n",
      "11  ch               Bern                    3.0   RF - City\n",
      "32  sk         Bratislava                    3.0  XGB - City\n",
      "22  de       Braunschweig                    3.0        Both\n",
      "19  au           Brisbane                    3.0   RF - City\n",
      "27  hu           Budapest                    3.0        Both\n",
      "2   ru        Chelyabinsk                    3.0        Both\n",
      "0   us             Dayton                    3.0   RF - City\n",
      "13  nl           Den Haag                    3.0        Both\n",
      "24  de            Dresden                    3.0        Both\n",
      "36  de         Dusseldorf                    3.0  XGB - City\n",
      "37  de             Erfurt                    3.0  XGB - City\n",
      "17  it            Firenze                    3.0        Both\n",
      "8   ru              Kazan                    3.0        Both\n",
      "31  ru        Krasnoyarsk                    3.0  XGB - City\n",
      "15  ar           La Plata                    3.0   RF - City\n",
      "12  ch         Le Sentier                    3.0   RF - City\n",
      "7   cz            Liberec                    3.0        Both\n",
      "26  pl               Lodz                    3.0        Both\n",
      "14  ch             Luzern                    3.0        Both\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add 'Model' column\n",
    "top_cities_predicted_rf = top_cities_predicted_rf.copy()\n",
    "top_cities_predicted_rf['Model'] = 'RF - City'\n",
    "\n",
    "top_cities_predicted_xgb = top_cities_predicted_xgb.copy()\n",
    "top_cities_predicted_xgb['Model'] = 'XGB - City'\n",
    "\n",
    "# Step 2: Ensure no duplicates before merging and take more cities from each model\n",
    "top_cities_predicted_rf = top_cities_predicted_rf.head(50).drop_duplicates(subset=['city', 'iso'])\n",
    "top_cities_predicted_xgb = top_cities_predicted_xgb.head(50).drop_duplicates(subset=['city', 'iso'])\n",
    "\n",
    "# Step 3: Merge RF and XGB predictions based on 'city' and 'iso'\n",
    "merged = pd.merge(\n",
    "    top_cities_predicted_rf, \n",
    "    top_cities_predicted_xgb, \n",
    "    on=['city', 'iso'],  # Ensure 'iso' is included in the merge\n",
    "    how='outer', \n",
    "    suffixes=('_rf', '_xgb')\n",
    ")\n",
    "\n",
    "# Step 4: Convert growth values to numeric to avoid TypeErrors\n",
    "merged['predicted_city_growth_rf'] = pd.to_numeric(merged['predicted_city_growth_rf'], errors='coerce')\n",
    "merged['predicted_city_growth_xgb'] = pd.to_numeric(merged['predicted_city_growth_xgb'], errors='coerce')\n",
    "\n",
    "# Step 5: Determine the correct 'Model' label\n",
    "merged['Model'] = merged.apply(\n",
    "    lambda row: 'Both' if not pd.isna(row['Model_rf']) and not pd.isna(row['Model_xgb'])\n",
    "    else row['Model_rf'] if not pd.isna(row['Model_rf']) \n",
    "    else row['Model_xgb'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 6: Use the highest predicted_city_growth from RF or XGB\n",
    "merged['predicted_city_growth'] = merged[['predicted_city_growth_rf', 'predicted_city_growth_xgb']].max(axis=1)\n",
    "\n",
    "# Step 7: Select only necessary columns, including 'iso'\n",
    "top_30_cities = merged[['iso', 'city', 'predicted_city_growth', 'Model']]\n",
    "\n",
    "# Step 8: Fill missing growth values (optional, depending on data)\n",
    "top_30_cities = top_30_cities.copy()\n",
    "top_30_cities.loc[:, 'predicted_city_growth'] = top_30_cities['predicted_city_growth'].fillna(0)\n",
    "\n",
    "# Step 9: Sort by predicted growth (descending) and then by city name\n",
    "top_30_cities = top_30_cities.sort_values(by=['predicted_city_growth', 'city'], ascending=[False, True])\n",
    "\n",
    "# Step 10: Select top 20 unique cities\n",
    "top_25_cities = top_30_cities.head(25)\n",
    "\n",
    "# Display results\n",
    "print(\"Top 25 Cities Predictions Alphabetically:\")\n",
    "print(top_25_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6920cfdc-a06c-482c-8ee7-87f1b55cb068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iso\n",
       "ar    1\n",
       "at    2\n",
       "au    2\n",
       "bg    2\n",
       "ca    1\n",
       "ch    5\n",
       "cz    1\n",
       "de    8\n",
       "fr    1\n",
       "hu    2\n",
       "it    3\n",
       "nl    1\n",
       "nz    1\n",
       "pl    1\n",
       "ru    4\n",
       "sk    1\n",
       "us    3\n",
       "uz    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_30_cities.groupby('iso').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "56ec44af-c2f2-4eda-80db-ade31e7baa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               city iso  city_change_from_previous_season\n",
      "748          Moscow  ru                           27911.0\n",
      "1216     Washington  us                           16128.0\n",
      "1062  St Petersburg  ru                            9681.0\n",
      "51        Arlington  us                            9648.0\n",
      "263        Columbus  us                            3549.0\n",
      "759         München  de                            3381.0\n",
      "931        Richmond  us                            3042.0\n",
      "846           Paris  fr                            2971.0\n",
      "868    Philadelphia  us                            2916.0\n",
      "327      Dusseldorf  de                            1677.0\n",
      "315         Dresden  de                            1674.0\n",
      "892     Portland OR  us                            1539.0\n",
      "62          Atlanta  us                            1312.0\n",
      "295          Denver  us                            1168.0\n",
      "371         Firenze  it                            1163.0\n",
      "1204       Voronezh  ru                             979.0\n",
      "290          Dayton  us                             968.0\n",
      "538         Jackson  us                             900.0\n",
      "642            Linz  at                             866.0\n",
      "687          Madrid  es                             861.0\n",
      "460           Halle  de                             820.0\n",
      "383       Frankfurt  de                             817.0\n",
      "95        Barcelona  es                             765.0\n",
      "869         Phoenix  us                             720.0\n",
      "605       Lancaster  us                             720.0\n"
     ]
    }
   ],
   "source": [
    "print(top_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "cd2eb3ed-2c3c-40ec-bcb0-2254da84c485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities and ISO codes existing in both datasets:\n",
      "         city iso\n",
      "0      Dayton  us\n",
      "1     Dresden  de\n",
      "2  Dusseldorf  de\n",
      "3     Firenze  it\n",
      "4      Moscow  ru\n",
      "5       Paris  fr\n",
      "6  Washington  us\n"
     ]
    }
   ],
   "source": [
    "common_cities = pd.merge(\n",
    "    top_30_cities[['city', 'iso']],  # Only keep relevant columns\n",
    "    top_cities[['city', 'iso']],     # Compare against full dataset\n",
    "    on=['city', 'iso'],              # Match on both city & iso\n",
    "    how='inner'                      # Keep only common pairs\n",
    ")\n",
    "\n",
    "print(\"Cities and ISO codes existing in both datasets:\")\n",
    "print(common_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "68a2b35c-3364-464c-9985-a177e81089a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>iso</th>\n",
       "      <th>country_change_from_previous_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>ru</td>\n",
       "      <td>194425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Italy</td>\n",
       "      <td>it</td>\n",
       "      <td>87941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>se</td>\n",
       "      <td>6459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Poland</td>\n",
       "      <td>pl</td>\n",
       "      <td>5653.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Spain</td>\n",
       "      <td>es</td>\n",
       "      <td>3115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>2892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Japan</td>\n",
       "      <td>jp</td>\n",
       "      <td>631.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Latvia</td>\n",
       "      <td>lv</td>\n",
       "      <td>529.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>ee</td>\n",
       "      <td>517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Slovenia</td>\n",
       "      <td>si</td>\n",
       "      <td>509.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Belarus</td>\n",
       "      <td>by</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>uz</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>am</td>\n",
       "      <td>442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>China</td>\n",
       "      <td>cn</td>\n",
       "      <td>396.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Serbia</td>\n",
       "      <td>rs</td>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>ar</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Korea, Republic of</td>\n",
       "      <td>kr</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Mongolia</td>\n",
       "      <td>mn</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>ie</td>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>dk</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>al</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>ge</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>mx</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>nz</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>kz</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country Name iso  country_change_from_previous_season\n",
       "58  Russian Federation  ru                             194425.0\n",
       "34               Italy  it                              87941.0\n",
       "65              Sweden  se                               6459.0\n",
       "55              Poland  pl                               5653.0\n",
       "64               Spain  es                               3115.0\n",
       "50         Netherlands  nl                               2892.0\n",
       "35               Japan  jp                                631.0\n",
       "39              Latvia  lv                                529.0\n",
       "23             Estonia  ee                                517.0\n",
       "62            Slovenia  si                                509.0\n",
       "6              Belarus  by                                450.0\n",
       "73          Uzbekistan  uz                                450.0\n",
       "2              Armenia  am                                442.0\n",
       "13               China  cn                                396.0\n",
       "59              Serbia  rs                                372.0\n",
       "1            Argentina  ar                                288.0\n",
       "37  Korea, Republic of  kr                                211.0\n",
       "49            Mongolia  mn                                180.0\n",
       "32             Ireland  ie                                177.0\n",
       "20             Denmark  dk                                165.0\n",
       "0              Albania  al                                115.0\n",
       "26             Georgia  ge                                111.0\n",
       "46              Mexico  mx                                 91.0\n",
       "51         New Zealand  nz                                 73.0\n",
       "36          Kazakhstan  kz                                 66.0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "88d613b9-e71a-4742-9482-affc3b770ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries that exist in both datasets:\n",
      "          Country Name\n",
      "0            Argentina\n",
      "1                China\n",
      "2              Denmark\n",
      "3              Estonia\n",
      "4              Ireland\n",
      "5                Italy\n",
      "6                Japan\n",
      "7   Korea, Republic of\n",
      "8               Latvia\n",
      "9               Mexico\n",
      "10         Netherlands\n",
      "11         New Zealand\n",
      "12              Poland\n",
      "13  Russian Federation\n",
      "14              Serbia\n",
      "15            Slovenia\n",
      "16               Spain\n",
      "17              Sweden\n",
      "18          Uzbekistan\n",
      "(19, 1)\n"
     ]
    }
   ],
   "source": [
    "common_countries = pd.merge(\n",
    "    top_30_countries[['Country Name']],  # Only keep relevant columns\n",
    "    top_countries[['Country Name']],     # Compare against full dataset\n",
    "    on=['Country Name'],              # Match on both country \n",
    "    how='inner'                      # Keep only common pairs\n",
    ")\n",
    "\n",
    "print(\"Countries that exist in both datasets:\")\n",
    "print(common_countries)\n",
    "print(common_countries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45a294-5a6c-44ea-a607-64e23b9b9fa9",
   "metadata": {},
   "source": [
    "After running multiple predictive models, the findings on the best emerging markets for opera show a strong correlation between predictive modeling results and visual data exploration. Among the top 30 countries (compiled from the best-performing country models), 19 countries overlap, reinforcing their potential as emerging markets.\n",
    "\n",
    "At the city level, the overlap is smaller, but a few standout cities emerge. A strategic approach for a traveling opera company could involve a balanced model: using well-established opera hubs as anchor locations during their off-seasons—such as Florence, Italy, and various Austrian cities, where seasonality trends differ from other European markets.\n",
    "\n",
    "This company could then take the same productions to emerging markets during peak seasons in their anchor cities, tapping into new audiences in places like Switzerland (which appears multiple times in the city rankings), the Czech Republic, Hungary, and Uzbekistan—markets that are slowly expanding and present lower stakes for early investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fdb12-f070-4c01-a6a6-5fc97c90887f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
